{
  
    
        "post0": {
            "title": "Predicting Collision Risk from Historical Collision Data Messages (CDMs) using Machine Learning",
            "content": "1. Introduction . The number of close approaches between space objects is increasing due to the proliferation of satellites and debris in orbit. It is a routine operation for satellite operators to avoid probable collisions by conducting maneuvers based on conjunction data messages (CDMs). The number of CDMs issued weekly are a couple of orders of magnitude larger than the actionable ones, and this puts pressure on satellite operators. Therefore, an automated process that can predict collision risk with desired accuracy a couple of days ahead to allow satellite operators to plan for collision avoidance maneuvers is desired. In this work, the feasibility of leveraging deep learning to predict the final collision risk between two space objects from the history of CDMs is investigated. The CDMs issued for the satellites that ESA&#39;s Space Debris Office supports are used to train an ensemble of Manhattan-LSTM models. The proposed ensemble model leverages the similarity scores between the input pairs instead of determining the decision boundary between high and low-risk close approach classes to address the imbalanced data. The significant imbalance and class overlapping in the data is also addressed by casting the problem as an anomaly detection problem and utilizing ensemble of various models. The proposed ensemble model with majority vote can predict collision risks two days ahead with a precision of 91% for high-risk cases (after adding up the predictions of two-stages). It should ne noted that the solution is two-stage. First stage involves feature engineering and casting the problem as anomaly detection problem by branching normal and anomalous cases with the most important feature, and that gives the precision score of 75% for high-risk cases. The second stage leverages the Manhattan-LSTM to decide whether each input pairs are similar, and this gives f1 score of 80% for anomaly detection of high-risk cases. Therefore, it is possible to determine low-risk to high-risk at close approach time anomalies. . 2. Data Preparation . Machine learning models are sensitive to the training data, and the attributes of the data should be understood before training models with them. The dataset used in this work includes time-series of CDMs issued for close approaches between a satellite and a space object. However, each example includes some information that exists in the standard CDM format, yet not all. The training dataset has 162634 rows (CDMs), 13154 of which are unique close approaches (approximately 12 CDMs for each event), and 103 columns(features from each CDM). Some features from CDMs are probability of collision risk, miss distance, relative position and velocity, solar activity indices,uncertainty in position and velocity, radar cross sections, and some orbitaldata about both space object. Further details about all features available in the dataset can be accessed on Kelvins platform [1]. Since the training dataset is small, 20 most informative features are used for training models to makesure the model doesn’t learn the noise in the data. . The training dataset has time-series of CDMs ranging from 7 days to a couple hours before the close approach. ESA Space Debris Office notifies flight teams to plan for possible maneuvers 2 days before the time of the closest approaches for their satellites, and they conduct futher analysis to decide whether to maneuver within 1 day. Since there exists uncertainty in orbit determination and propagation of space objects, it is preffered to start planning for collision avoidance maneuvers a couple of days ahead of the close approach time to keep uncertainty small and let flight teams have enough time to analyse the maneuvers to be conducted. . Since it is desired to predict the collision risk two or more days ago, the latest available collision risk value (target label) and a CDM (input features) which is issued two or more days before the close approach should be selected for each close approach event. Figure 1 shows how some input features of a CDM and a collision risk value as a target label (log base 10) are selected as described above to form a candidate input-output pair (most informative features are selected during feature engineering) for training machine learning models from time-series of CDMs. . . Figure 1: The selection of input features and the associated collision risk value as target (log base 10).Due to my domain expertise, I directly investigated whether the latest available collision risk values that are issued 2 or more days before the close approach are enough to predict the target risk class accurately, and I realised this was the case. Collision risk is derived from other input features, such as miss distance, position covariance and physical sizes of space objects, and it is an important feature. However, there are 23 cases that are turn out to be high-risk and 269 cases that turn out to be low-risk when the latest available collision risk values predict low-risk and high-risk respectively. Therefore, this work casts the collision risk prediction problem as an anomaly detection problem. It investigates the feasibility of using machine learning to predict collision risk class (high or low-risk) with desired accuracy by detecting anomalies (Figure 2). . . Figure 2: The outline of sample sizes when the normal and anomaly groups are splitted by the latest available collision risk value.The dataset has many challenges due to its small size, and it is more sensitive to the quality of data in general. Imbalance ratio, which is the ratio of the majority to the minority class, is 370 for low-risk target class and 3 for high-risk target class. Anomalies for low-risk target class has only 23 examples which is a challenge to even split data for validation purposes because the validation dataset should have enough examples for results to be statistically meaningful. In addition, the total number of samples for high-risk target class is 339. The stratified 3-folds cross-validator is implemented to avoid overfitting. An ensemble of various simple models with different hyperparameters that determines the similarity in input pairs is leveraged for better generalization. Figure shows the 2-D feature space representation of some input features (features that fuse multiple other features), namely time to close approach, mahalanobis distance, maximum risk estimate and scaling, and target labels using t-distributed stochastic neighbor embedding (t-SNE) algorithm [2]. It should be noted that due to significant class imbalance and class overlapping high-risk to low-risk anomalies, no statistical model can be developed for that case. . . Figure 3: T-distributed stochastic neighbor embedding representation of low-risk (left) and high-risk (right) target classes. 3. Manhattan-LSTM Model . There are two neural networks in Manhattan-LSTM model, and they share weights [4]. The model utilizes LSTM to read the features that are derived from CDMs. Then, the similarity score between the input pairs is computed by using the similarity function $g = exp(-|h_{a}-h_{b}|_{1})$ (Figure 4). The LSTM is chosen because it allows variable lengths (to allow missing data to be included) as inputs, and this is not because the inputs are time-series. The total number of pairs for training is 1 million (normal-abnormal), and 10000 pairs are prepared for validation. . . Figure 4: The outline of Manhattan-LSTM model.Table 1 shows the features that are selected based on the distribution differences between normal and anomalous case. . Features Variable Description . Time to tca | time_to_tca | Time interval between CDM creation and time-of-closest approach (days) | . Max risk estimate | max_risk_estimate | Maximum collision probability obtained by scaling combined covariance | . Mahalonobis distance | mahalanobis_distance | Miss distance scaled with uncertainty | . Miss distance | miss_distance | Relative position between chaser &amp; target at tca (m) | . Number of CDMs issued | no_larger_2 | Number of CDMs issued before 2 days to the tca | . Mean of risk values | mean_larger_2 | Mean of collsion risk values for the CDMs issued before 2 days to the tca | . STD of risk values | std_larger_2 | STD of collsion risk values for the CDMs issued before 2 days to the tca | . Debris positional uncertainty (cross-track) | c_sigma_n | Covariance (cross-track) position standard deviation (sigma) in meters | . Debris positional uncertainty (along-track) | c_sigma_t | Covariance transverse (along-track) position standard deviation (sigma) in meters | . Debris positional uncertainty (radial) | c_sigma_r | Covariance radial position standard deviation (sigma) in meters | . Debris positional covariance determinant | c_position_covariance_det | Determinant of covariance | . Debris number of observations used | c_obs_used | Number of observations used for orbit determination (per CDM) | . Table 1 : Features selected as inputs for Manhattan-LSTM.Table 2 shows the hyperparameters that are used to train the Manhattan-LSTM. . Hyperparameters Values . Hidden layer size | 32 | . Batch size | 64 | . Epoch number | 100 | . Activation function | tanh | . Dropout | 0.05 | . Learning rate | 2e-5 | . Table 2 : The hyperparameters that are used to train the Manhattan-LSTM. 4. Results . Figure 5 shows the loss values for both training and validation data, and Figure 5 shows the area-under-curve values during training. . . Figure 5: Loss values for both training and validation data during training. . Figure 6: AUC values for both training and validation data during training.In Figure 7, correlation matrix for the test data is presented using the model for the epoch 55, which is the epoch when overfitting starts. . . Figure 6: AUC values for both training and validation data during training. 5. References . [1] https://kelvins.esa.int/collision-avoidance-challenge/ . [2] Maaten, L.V.D. and Hinton, G., 2008. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), pp.2579-2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html . [3] Mueller, J. and Thyagarajan, A., 2016, March. Siamese recurrent architectures for learning sentence similarity. In thirtieth AAAI conference on artificial intelligence. https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewPaper/12195 . 6. Code . # Load the required libraries from sklearn.metrics import fbeta_score import numpy as np from matplotlib import pyplot as plt import pandas as pd import seaborn as sns import os import random from sklearn.preprocessing import LabelEncoder from sklearn.metrics import mean_squared_error from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import KFold from sklearn.manifold import TSNE from sklearn.decomposition import PCA from time import time from sklearn.model_selection import train_test_split import itertools import datetime from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda, Dense, Conv1D, Flatten, Dropout import tensorflow.keras.backend as K from tensorflow.keras.optimizers import Adadelta, Adam, SGD from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.metrics import AUC from sklearn.manifold import TSNE from datagen import prepare_data pd.set_option(&#39;display.max_columns&#39;, None) . #prepare the data to test challenge metric #define paths path_to_dataset = &#39;dataset/&#39; #read the training data train = pd.read_csv(os.path.join(path_to_dataset,&#39;train_data.csv&#39;)) test = pd.read_csv(os.path.join(path_to_dataset,&#39;test_data.csv&#39;)) #Shapes of the train and test dataset print(&quot;shape of train data {}&quot;.format(train.shape)) print(&quot;shape of test data {}&quot;.format(test.shape)) . #train time to close approach min and max train[&quot;time_to_tca&quot;].describe().loc[[&quot;min&quot;,&quot;max&quot;]] . #train time to close approach min and max test[&quot;time_to_tca&quot;].describe().loc[[&quot;min&quot;,&quot;max&quot;]] . print(&quot;number of events that has less than 2.0 days data : &quot;,len(train[train[&quot;time_to_tca&quot;]&lt;2.0].groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) print(&quot;number of events that has more than 2.0 days data : &quot;,len(train[train[&quot;time_to_tca&quot;]&gt;2.0].groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) print(&quot;total number of events : &quot;,len(train.groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) events_less_than_two_days = train[train[&quot;time_to_tca&quot;]&lt;2.0].groupby(&quot;event_id&quot;)[&quot;event_id&quot;].first().tolist() events_more_than_two_days = train[train[&quot;time_to_tca&quot;]&gt;2.0].groupby(&quot;event_id&quot;)[&quot;event_id&quot;].first().tolist() . # number of events that has time_to_tca larger and smaller than 2 at the same time events_trainable = [value for value in events_less_than_two_days if value in events_more_than_two_days] print(&quot;number of trainable events : &quot;,len(events_trainable)) . #generate new training data that uses last available CDM (closest to 2 days) and adds latest CDM risk as target variable lenData = len(events_trainable) new_train = [] target_variable = [] no_larger_2 = [] mean_larger_2 =[] std_larger_2 = [] mean_nan_number =[] std_nan_number = [] for cnt in range(lenData): new_train.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)][-1:].values.tolist()[0]) target_variable.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])][-1:].risk.values[0]) no_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].shape[0]) mean_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].risk.mean()) std_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].risk.std(ddof=0)) mean_nan_number.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].isnull().sum(axis=1).mean()) std_nan_number.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].isnull().sum(axis=1).std(ddof=0)) new_traintrain_pd = pd.DataFrame(new_train,columns=train.columns.tolist()) #add target value to the dataset target_variable_pd = pd.DataFrame(target_variable,columns=[&quot;target_risk&quot;]) no_larger_2_pd = pd.DataFrame(no_larger_2,columns=[&quot;no_larger_2&quot;]) mean_larger_2_pd = pd.DataFrame(mean_larger_2,columns=[&quot;mean_larger_2&quot;]) std_larger_2_pd = pd.DataFrame(std_larger_2,columns=[&quot;std_larger_2&quot;]) mean_nan_number_pd = pd.DataFrame(mean_nan_number,columns=[&quot;mean_nan_number&quot;]) std_nan_number_pd = pd.DataFrame(std_nan_number,columns=[&quot;std_nan_number&quot;]) new_traintrain_pd[&quot;target_risk&quot;] = target_variable_pd.values new_traintrain_pd[&quot;no_larger_2&quot;] = no_larger_2_pd.values new_traintrain_pd[&quot;mean_larger_2&quot;] = mean_larger_2_pd.values new_traintrain_pd[&quot;std_larger_2&quot;] = std_larger_2_pd.values new_traintrain_pd[&quot;mean_nan_number&quot;] = mean_nan_number_pd.values new_traintrain_pd[&quot;std_nan_number&quot;] = std_nan_number_pd.values new_traintrain_pd.head(5) . # Determines the high risk events new_traintrain_pd[&#39;target_risk_class&#39;]=np.nan new_traintrain_pd.loc[new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0,&#39;target_risk_class&#39;] = int(1) new_traintrain_pd.loc[new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0,&#39;target_risk_class&#39;] = int(0) print(&quot;number of high risk events&quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==1].shape[0]) print(&quot;number of low risk events&quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==0].shape[0]) print(&quot;ratio of high risk versus low risk &quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==1].shape[0]/new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==0].shape[0]*100) . # Uses the latest available risk value to determine the normal and anomalous cases new_traintrain_pd[&#39;predicted_risk&#39;] = 0 new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;]&lt;-6.0,&#39;predicted_risk&#39;] = -6.000000000000001 new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;]&gt;=-6.0,&#39;predicted_risk&#39;] = new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;] &gt;=-6.0].risk.values . # Assigns the classes based on risk values new_traintrain_pd[&#39;predicted_risk_class&#39;]=np.nan new_traintrain_pd.loc[new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0,&#39;predicted_risk_class&#39;] = int(1) new_traintrain_pd.loc[new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0,&#39;predicted_risk_class&#39;] = int(0) print(&quot;number of high risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==1].shape[0]) print(&quot;number of low risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==0].shape[0]) print(&quot;ratio of high risk versus low risk with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==1].shape[0]/new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==0].shape[0]*100) . new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]=np.nan new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0),&#39;predicted_risk_anomaly&#39;] = int(0) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0),&#39;predicted_risk_anomaly&#39;] = int(1) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0),&#39;predicted_risk_anomaly&#39;] = int(2) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0),&#39;predicted_risk_anomaly&#39;] = int(3) print(&quot;number of 0 to 1 risk anomaly events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==3].shape[0]) print(&quot;number of 1 to 0 risk anomaly events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==2].shape[0]) print(&quot;number of 0 to 0 risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==1].shape[0]) print(&quot;number of 1 to 1 risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==0].shape[0]) . #convert categorical c_object_type le = LabelEncoder() new_traintrain_pd[&quot;c_object_type&quot;] = le.fit_transform(new_traintrain_pd[&quot;c_object_type&quot;]) . # Computes the missing value percentage obj = new_traintrain_pd.isnull().sum() for key,value in obj.iteritems(): if value!=0: print(&quot;key : {} -&gt; missing data (percentage) :{} %&quot;.format(key, value/8892.0*100)) . # Fills missing values with mean values and checks whether there exist more missing data new_traintrain_pd.fillna(new_traintrain_pd.mean(), inplace=True) obj = new_traintrain_pd.isnull().sum() for key,value in obj.iteritems(): if value!=0: print(&quot;key : {} -&gt; missing data (percentage) :{} %&quot;.format(key, value/8892.0*100)) . # Scales the input data for standard scaler from sklearn.preprocessing import StandardScaler qt_risk = StandardScaler() new_traintrain_pd[&#39;qt_risk&#39;] = qt_risk.fit_transform(np.expand_dims(new_traintrain_pd[&#39;risk&#39;].values,axis=1)) # new_test_pd[&#39;qt_risk&#39;] = qt_risk.transform(np.expand_dims(new_test_pd[&#39;risk&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_risk&#39;]) qt_time_to_tca = StandardScaler() new_traintrain_pd[&#39;qt_time_to_tca&#39;] = qt_time_to_tca.fit_transform(np.expand_dims(new_traintrain_pd[&#39;time_to_tca&#39;].values,axis=1)) # new_test_pd[&#39;qt_time_to_tca&#39;] = qt_time_to_tca.transform(np.expand_dims(new_test_pd[&#39;time_to_tca&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_time_to_tca&#39;]) qt_max_risk_estimate = StandardScaler() new_traintrain_pd[&#39;qt_max_risk_estimate&#39;] = qt_max_risk_estimate.fit_transform(np.expand_dims(new_traintrain_pd[&#39;max_risk_estimate&#39;].values,axis=1)) # new_test_pd[&#39;qt_max_risk_estimate&#39;] = qt_max_risk_estimate.transform(np.expand_dims(new_test_pd[&#39;max_risk_estimate&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_max_risk_estimate&#39;]) qt_max_risk_scaling = StandardScaler() new_traintrain_pd[&#39;qt_max_risk_scaling&#39;] = qt_max_risk_scaling.fit_transform(np.expand_dims(new_traintrain_pd[&#39;max_risk_scaling&#39;].values,axis=1)) # new_test_pd[&#39;qt_max_risk_scaling&#39;] = qt_max_risk_scaling.transform(np.expand_dims(new_test_pd[&#39;max_risk_scaling&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_max_risk_scaling&#39;]) qt_mahalanobis_distance = StandardScaler() new_traintrain_pd[&#39;qt_mahalanobis_distance&#39;] = qt_mahalanobis_distance.fit_transform(np.expand_dims(new_traintrain_pd[&#39;mahalanobis_distance&#39;].values,axis=1)) # new_test_pd[&#39;qt_mahalanobis_distance&#39;] = qt_mahalanobis_distance.transform(np.expand_dims(new_test_pd[&#39;mahalanobis_distance&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_mahalanobis_distance&#39;]) qt_miss_distance = StandardScaler() new_traintrain_pd[&#39;qt_miss_distance&#39;] = qt_miss_distance.fit_transform(np.expand_dims(new_traintrain_pd[&#39;miss_distance&#39;].values,axis=1)) # new_test_pd[&#39;qt_miss_distance&#39;] = qt_miss_distance.transform(np.expand_dims(new_test_pd[&#39;miss_distance&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_miss_distance&#39;]) qt_no_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_no_larger_2&#39;] = qt_no_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;no_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_no_larger_2&#39;] = qt_no_larger_2.transform(np.expand_dims(new_test_pd[&#39;no_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_no_larger_2&#39;]) qt_mean_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_mean_larger_2&#39;] = qt_mean_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;mean_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_mean_larger_2&#39;] = qt_mean_larger_2.transform(np.expand_dims(new_test_pd[&#39;mean_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_mean_larger_2&#39;]) qt_std_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_std_larger_2&#39;] = qt_std_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;std_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_std_larger_2&#39;] = qt_std_larger_2.transform(np.expand_dims(new_test_pd[&#39;std_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_std_larger_2&#39;]) qt_c_sigma_n = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_n&#39;] = qt_c_sigma_n.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_n&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_n&#39;] = qt_c_sigma_n.transform(np.expand_dims(new_test_pd[&#39;c_sigma_n&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_n&#39;]) qt_c_sigma_t = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_t&#39;] = qt_c_sigma_t.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_t&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_t&#39;] = qt_c_sigma_t.transform(np.expand_dims(new_test_pd[&#39;c_sigma_t&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_t&#39;]) qt_c_sigma_r = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_r&#39;] = qt_c_sigma_r.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_r&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_r&#39;] = qt_c_sigma_r.transform(np.expand_dims(new_test_pd[&#39;c_sigma_r&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_r&#39;]) qt_c_position_covariance_det = StandardScaler() new_traintrain_pd[&#39;qt_c_position_covariance_det&#39;] = qt_c_position_covariance_det.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_position_covariance_det&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_position_covariance_det&#39;] = qt_c_position_covariance_det.transform(np.expand_dims(new_test_pd[&#39;c_position_covariance_det&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_position_covariance_det&#39;]) qt_c_obs_used = StandardScaler() new_traintrain_pd[&#39;qt_c_obs_used&#39;] = qt_c_obs_used.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_obs_used&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_obs_used&#39;] = qt_c_obs_used.transform(np.expand_dims(new_test_pd[&#39;c_obs_used&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_obs_used&#39;]) . # Prepares the data for Manhattan-LSTM X_train, Y_train, X_train_val, Y_train_val, new_data_pd = prepare_data(new_traintrain_pd) . # Model variables n_hidden = 32 gradient_clipping_norm = 0.5 batch_size = 64 n_epoch = 100 def exponent_neg_manhattan_distance(left, right): &#39;&#39;&#39; Helper function for the similarity estimate of the LSTMs outputs&#39;&#39;&#39; return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)) left_input = Input(shape=(len(new_data_pd.feature1[0]),1), dtype=&#39;float32&#39;) right_input = Input(shape=(len(new_data_pd.feature1[0]),1), dtype=&#39;float32&#39;) print(left_input.shape) # Since this is a siamese network, both sides share the same LSTM shared_lstm = LSTM(n_hidden,activation=&#39;tanh&#39;,dropout=0.05) left_output = shared_lstm(left_input) right_output = shared_lstm(right_input) # Calculates the distance as defined by the MaLSTM model malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output]) # Pack it all up into a model malstm = Model([left_input, right_input], [malstm_distance]) optimizer = Adam(learning_rate=0.00002,decay=0.000002) malstm.compile(loss=&#39;mean_squared_error&#39;, optimizer=optimizer, metrics=[AUC()]) # Start training training_start_time = time() # Include the epoch in the file name (uses `str.format`) checkpoint_path = &quot;cp_epoch_{epoch:04d}.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model&#39;s weights every 5 epochs cp_callback = ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5) malstm_trained = malstm.fit([X_train[&#39;left&#39;], X_train[&#39;right&#39;]], Y_train, batch_size=batch_size, nb_epoch=n_epoch, validation_data=([X_train_val[&#39;left&#39;], X_train_val[&#39;right&#39;]], Y_train_val),callbacks=[cp_callback]) print(&quot;Training time finished. n{} epochs in {}&quot;.format(n_epoch, datetime.timedelta(seconds=time()-training_start_time))) . sns.set() plt.plot(malstm_trained.history[&#39;loss&#39;],&#39;r-.&#39;,label=&#39;loss&#39;) plt.plot(malstm_trained.history[&#39;val_loss&#39;],&#39;g-*&#39;,label=&#39;val_loss&#39;) plt.legend() . plt.plot(malstm_trained.history[&#39;auc&#39;],&#39;r-.&#39;,label=&#39;auc&#39;) plt.plot(malstm_trained.history[&#39;val_auc&#39;],&#39;g-*&#39;,label=&#39;val_auc&#39;) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Auc&#39;) plt.legend() . from sklearn.metrics import confusion_matrix from matplotlib import pyplot as plt import itertools def plot_confusion_matrix(cm,classes,normalize=False,title=&#39;confusion matrix&#39;,cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;)/cm.sum(axis=1)[:,np.newaxis] print(&#39;normalized&#39;) else: print(&#39;not normalized&#39;) # print(cm) plt.imshow(cm,interpolation=&#39;nearest&#39;,cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(-1,len(classes)) plt.xticks(tick_marks,np.array(classes)-1,rotation=45) plt.yticks(tick_marks,np.array(classes)-1) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max()/2.0 for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])): plt.text(j,i,format(cm[i,j],fmt),horizontalalignment=&#39;center&#39;,color=&#39;white&#39; if cm[i,j]&gt;thresh else &#39;black&#39;) plt.tight_layout() plt.ylabel(&#39;True Label&#39;) plt.xlabel(&#39;Predicted Label&#39;) plt.show() . path = &#39;cp_epoch_0055.ckpt&#39; malstm.load_weights(path) . p_test = malstm.predict([X_train_val[&#39;left&#39;], X_train_val[&#39;right&#39;]]) p_test_pd = pd.DataFrame(p_test,columns=[&#39;prob&#39;]) . p_test_pd.loc[p_test_pd[&#39;prob&#39;]&gt;=0.5,&#39;prob&#39;] = int(1) . p_test_pd.loc[p_test_pd[&#39;prob&#39;]&lt;0.5,&#39;prob&#39;] = int(0) . cm = confusion_matrix(Y_train_val,p_test_pd.prob) . plot_confusion_matrix(cm,list(range(3))) . import sklearn sklearn.metrics.f1_score(Y_train_val,p_test_pd.prob) .",
            "url": "https://rasitabay.github.io/blog/collision%20risk/satellites/deep%20learning/manhattan-lstm/2020/04/22/ESA-Collision-Risk-Prediction.html",
            "relUrl": "/collision%20risk/satellites/deep%20learning/manhattan-lstm/2020/04/22/ESA-Collision-Risk-Prediction.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "High Precision 6D Orbit Propagator",
            "content": "1. Orbit Propagator . This blog post introduces a high precision 6D orbit propagator that I developed for my research. This 6D orbit propagator leverages Orekit low-level spaceflight dynamics library (3D dynamics only) and CNES VTS (Celestia) visualization tool. Below is the structure of the proposed 6D flight dynamics library (Figure 1). . . Figure 1: Layout of the proposed high precision 6D orbit propagator.There are 6 classes that handles the associated tasks. State class contains all functions and variables that defines the physical shape, size, and material properties of space objects, celestial bodies, frame definitions and conversions. Force class includes functions that compute the conservative, such as non-aspherical gravitational potential and third-body mass, and non-conservative forces, such as solar radiation pressure and air drag. Torque class contains functions for computing gravity gradient torque for gravitational forces and solar radiation pressure and air drag torque for non-gravitational forces. Propagator class includes functions to propagate the orbital equations of motion in Newtonian formalism using adaptive numerical integrators such as Runge-Kutta integrator of order 8 or Dormand and Prince of order 8. Visibility class contains the functions for various methods to compute the bidirectional reflectance distribution function (BRDF) of the space objects based on visibility geometry. All reference frames use the IERS 2010 conventions, and earth orientation parameters (EOP) are updated regularly. . 2. Demo Setup . For the demo, two extreme cases in terms of sensitivity of the space objects, namely massive cannonball and a small MLI-type material that is wrinkled, to the non-gravitational forces are presented in the demo. Cannonball is less sensitive to the non-gravitational forces, whereas non-gravitational forces and torques drastically affect the attitude/orbit evolution of high area-to-mass ratio wrinkled sheet, which is composed of 10 flat surfaces. Figure 2 shows the shapes of the space objects that are selected for the demo. Table 1 shows some of the physical parameters used for the demo. In Table 1, $d$ and $s$ are fractions of the diffuse and specular reflection ($s+d=1$), $ rho$ and $F_{0}$ are diffuse reflectance and specular reflectance of the surface at normal incidence, and $n_{u}$ and $n_{v}$ are phong-like exponents that control the shape of the specular lobe. . . Figure 2 : Shapes of cannonball and HAMR object that are used for the demo. Properties Cannonball HAMR object . Mass | 7850 kg | 0.0001125 kg | . Area | 1.209 $m^2$ | 0.0025 $m^2$ (each facet) | . Sunlight intensity | 455 $W/m^2$ | 455 $W/m^2$ | . d | 1.0 | 0.5 | . s | 0.0 | 0.5 | . $ rho$ | 0.26 | 0.26 | . $F_{0}$ | - | 0.5 | . $n_{u}$ | - | 0.5 | . $n_{v}$ | - | 0.5 | . Table 1 : Physical model parameters used for the space objects.For the initial orbit, the orbital elemets of a satellite (OPS 5111) are used by transforming them from TEME to J2000 reference frames. First, a two-line element set is fetched from spacetrack.org, and then the state (cartesian coordinates) at epoch is converted to J2000. The TLE used is , . 0 OPS 5111 . 1 10684U 78020A 20110.24021950 +.00000083 +00000-0 +00000-0 0 9994 . 2 10684 063.0042 120.7837 0079708 206.3069 153.3306 01.98073183291879 . The air drag coefficient is $2.8$ (for both objects) and aspherical gravitational potential order and degree are 20. The initial rotation rate for the HAMR object is $0.5$ degrees/s around the orbital normal axis. . 3. Results . Figure 3 and Figure 4 show the light intensity observed by the ground telescope for the hamr object and the cannonball respectively. Figure 5 shows the relative orbital evolution starting from the epoch (1 day propagation starting at 2020-04-19T05:45:54.964 (UTC)). Please note that the time window for the visibility starts (for the location at lat: $57.0^{ circ}$ and long: $-5.0^{ circ}$) at 2020-04-19T21:17:04.964 (UTC) and ends at 2020-04-19T22:55:24.964 (UTC). . . Figure 3 : Light intensity observed by the ground telescope for hamr object. . Figure 4 : Light intensity observed by the ground telescope for cannonball. . Figure 5 : Relative orbital evolution of space objects.The 3D visualisation for the relative motion (orbits diverge because hamr object is drastically perturbed by the non-gravitational forces) can be reached from the link, . https://drive.google.com/file/d/1FeZUaFtIT5W8D5UL9v2Mr_PMCZL0Nqz8/view?usp=sharing . And the visible pass in 3D (visible pass occurs when the ground observer is in dark and space object is in sun light) can be reached from the link, . https://drive.google.com/file/d/14F6Q6MH2FCJmfes40f9W3-kjYU7av1Ee/view?usp=sharing. . 4. References . [1] Space-track.org (fetched TLE for the initial orbit), https://www.space-track.org/auth/login . [2] Orekit (used for frame transformations and gravitational potential computations), https://www.orekit.org/ . [3] CNES VTS (used for 3D visualisation), https://timeloop.fr/vts/ . [4] Ashikhmin, Michael and Shirley, Peter, An Anisotropic Phong BRDF Model, Journal of Graphics Tools, 2000. . 5. Code . The code used to generate above analysis is provided below. . # Call the required libraries import seaborn as sns import numpy as np from matplotlib import pyplot from state import State from visibility import LambertianSphere # Replace ashikhminPremoze with LambertianSphere for cannonball from forces import NeutralDrag from forces import HolmesFeatherstoneGravity from forces import ThirdBodyForce from forces import ImprovedRadiation from torques import Torques from torques import GravityGradient from torques import MagneticDipole from propagator import NumericalPropagator from sgp4.io import twoline2rv from sgp4.earth_gravity import wgs84 import utils as utl . # Initial orbit is transformed from TLE mean elements in TEME to cartesian states in J2000. TLE=[] TLE.append(&#39;0 OPS 5111&#39;) TLE.append(&#39;1 10684U 78020A 20110.24021950 +.00000083 +00000-0 +00000-0 0 9994&#39;) TLE.append(&#39;2 10684 063.0042 120.7837 0079708 206.3069 153.3306 01.98073183291879&#39;)#Eccentricity modified satellite = twoline2rv( TLE[1],TLE[2],wgs84) tlepos = satellite.propagatemin(0)[0] tlevel = satellite.propagatemin(0)[1] tleTime = str(satellite.epoch.strftime(&quot;%Y-%m-%dT%H:%M:%S.%f&quot;)[0:-3]) state = np.array([tlepos[0],tlepos[1],tlepos[2],tlevel[0],tlevel[1],tlevel[2]])*1000.0 stateNew = utl.rotateFrame(tleTime,state,&#39;TEME&#39;,&#39;J2000&#39;) . # The configuration for Steel cannonball all diffusive and HAMR object with diffuse and specular reflection spacecraftConfig = {} spacecraftConfig[&#39;name&#39;] = &#39;Steel&#39; spacecraftConfig[&#39;type&#39;] = &#39;3DoF&#39;#6DoF or 3DoF spacecraftConfig[&#39;mass&#39;] = 7850#kg spacecraftConfig[&#39;date&#39;] = tleTime#UTC time spacecraftConfig[&#39;orbit&#39;] = np.array([stateNew[0] ,stateNew[1] ,stateNew[2], stateNew[3] ,stateNew[4] ,stateNew[5]])#PV in m and m/s (J2000) # Below is requred for 3DoF propagation - change type to 3DoF as well spacecraftConfig[&#39;radius&#39;] = 0.6204#m spacecraftState = State(spacecraftConfig) # # Below is the configuration for a HAMR object # spacecraftConfig[&#39;name&#39;] = &#39;Hamr&#39; # spacecraftConfig[&#39;type&#39;] = &#39;6DoF&#39;#6DoF or 3DoF # spacecraftConfig[&#39;mass&#39;] = 0.0001125#kg # spacecraftConfig[&#39;date&#39;] = tleTime#UTC time # spacecraftConfig[&#39;orbit&#39;] = np.array([stateNew[0] ,stateNew[1] ,stateNew[2], # stateNew[3] ,stateNew[4] ,stateNew[5]])#PV in m and m/s (J2000) # # Below is requred for 3DoF propagation - change type to 3DoF as well # spacecraftConfig[&#39;radius&#39;] = 0.3#m # # Below is requred for 6DoF propagation - change type to 6DoF as well # spacecraftConfig[&#39;attitude&#39;] = np.array([20.0,20.0,0.0,0.0,0.0,0.5])#omega and omega dot w.r.t orbit frame # spacecraftConfig[&#39;moi&#39;] = np.array([[1.54180631e-07, 0.0, -1.08223763e-07], # [0.0, 2.31268163e-07, 0.0], # [-1.08223763e-07, 0.0, 8.41187813e-08]])#m^2 w.r.t body frame # spacecraftConfig[&#39;magDipole&#39;] = np.array([0.01,0.01,0.01])#Nm/Tesla = Am^2 # spacecraftConfig[&#39;comOffset&#39;] = np.array([0.0,0.0,0.0])#m w.r.t body frame # spacecraftConfig[&#39;centerOfPressure&#39;] = np.array([[0.0,0.0,0.0558 ,0.0558 ,0.02165 ,0.02165 , # -0.02165,-0.02165,-0.0558 ,-0.0558], # [0.0,0.0,0.0 ,0.0 ,0.0 ,0.0 , # 0.0 ,0.0 ,0.0 ,0.0], # [0.0,0.0,0.07165,0.07165,0.0375 ,0.0375 , # -0.0375 ,-0.0375 ,-0.07165 ,-0.07165]]) # # location of facet center w.r.t body frame - meters # spacecraftConfig[&#39;areaOfFacets&#39;] = np.array([0.0025,0.0025,0.0025,0.0025,0.0025,0.0025,0.0025,0.0025, # 0.0025,0.0025]) # # area of each facet - meters^2 # spacecraftConfig[&#39;orientationOfFacets&#39;] = np.array([[0.0,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ], # [0.0,180.0,30.0 ,210.0,60.0,240.0,60.0,240.0,30.0,210.0], # [0.0,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ]]) # # orientation of the facet w.r.t body frame - degrees # spacecraftState = State(spacecraftConfig) . # Defines the airdrag parameters dragCoeff = 2.8 dragNeutral = NeutralDrag(spacecraftState,dragCoeff) . # Defines the aspherical gravitational potential parameters order = 20 degree = 20 gravity = HolmesFeatherstoneGravity(spacecraftState,order,degree) . # Defines the third-bosy mass interaction parameters sunBody = spacecraftState.sun moonBody = spacecraftState.moon thirdBodies = ThirdBodyForce(spacecraftState,[sunBody,moonBody]) . # Defines the visibility parameters gs = [57.0, -5.0, 606.0]#latitude, longitude, altitude brdfCoeff = {} brdfCoeff[&#39;cSunVis&#39;] = 455#W/m^2 brdfCoeff[&#39;d&#39;] = 0.5 brdfCoeff[&#39;rho&#39;] = 0.26 brdfCoeff[&#39;s&#39;] = 0.5 brdfCoeff[&#39;Fo&#39;] = 0.5 brdfCoeff[&#39;nu&#39;] = 0.5 brdfCoeff[&#39;nv&#39;] = 0.5 # AshikhminPremoze = ashikhminPremoze(spacecraftState,gs,brdfCoeff) # Below is when cannonball option LambertianSphere = LambertianSphere(spacecraftState,gs,brdfCoeff) . # SRPClassical = ClassicalRadiation(spacecraftState,brdfCoeff) . # Initializes the SRP force class emissivity = 0.05 SRPImproved = ImprovedRadiation(spacecraftState,brdfCoeff,emissivity) . # Initializes the torque classes # aeroTorque = Torques(spacecraftState,dragNeutral) # radiationTorque = Torques(spacecraftState,SRPImproved) # ggTorque = GravityGradient(spacecraftState,[])#No need for a force Model # magneticTorque = MagneticDipole(spacecraftState,[])#No need for a force Model . # Defines the propagator #prop = NumericalPropagator(8640.0,100.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPClassical], # [aeroTorque,radiationTorque,ggTorque,magneticTorque]) # prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPImproved], # [aeroTorque,radiationTorque],ashikhminPremoze) # prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPImproved], # [],AshikhminPremoze) prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, [dragNeutral,gravity,thirdBodies,SRPImproved], [],LambertianSphere) . # Propagate the orbit/attitude prop.Propagate() . # Keep steel data dataSteel = prop.sol[&#39;state&#39;][:,:6] # dataHamr = prop.sol[&#39;state&#39;][:,:6] . # Plots the flux seen by the ground observer selected = prop.sol[&#39;visibility&#39;][:,2]&gt;np.zeros(len(prop.sol[&#39;visibility&#39;][:])) desired = np.where(selected) startIndex = desired[0][0] endIndex = desired[0][-1] sns.set() print(&#39;Below plot is flux when start time : &#39; +str(prop.sol[&#39;time&#39;][startIndex]) + &#39; and end time : &#39;+str(prop.sol[&#39;time&#39;][endIndex])+&#39; (Cannonball object)&#39;) pyplot.plot(prop.sol[&#39;visibility&#39;][startIndex:endIndex,1]) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Flux (W/m^2)&#39;) pyplot.show() . Below plot is flux when start time : 2020-04-19T21:17:04.964 and end time : 2020-04-19T22:55:24.964 (Cannonball object) . # Plots the flux seen by the ground observer selected = prop.sol[&#39;visibility&#39;][:,2]&gt;np.zeros(len(prop.sol[&#39;visibility&#39;][:])) desired = np.where(selected) startIndex = desired[0][0] endIndex = desired[0][-1] sns.set() print(&#39;Below plot is flux when start time : &#39; +str(prop.sol[&#39;time&#39;][startIndex]) + &#39; and end time : &#39;+str(prop.sol[&#39;time&#39;][endIndex])+&#39; (HAMR object)&#39;) pyplot.plot(prop.sol[&#39;visibility&#39;][startIndex:endIndex,1]) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Flux (W/m^2)&#39;) pyplot.show() . Below plot is flux when start time : 2020-04-19T21:17:04.964 and end time : 2020-04-19T22:55:24.964 (HAMR object) . # Compute the relative orbit trajectory lenData = dataHamr.shape[0] relTrajectory = np.zeros((lenData,3)) posvel = np.zeros(6) for cnt1 in range(lenData): posvel[0:3] = dataSteel[cnt1,:3] posvel[3:6] = dataSteel[cnt1,3:] rotMatIne2Orb = utl.OrbitFrameAxes(posvel).T relTrajectory[cnt1,:] = np.dot(rotMatIne2Orb,dataHamr[cnt1,:3]-dataSteel[cnt1,:3])/1000.0 . # Plots the relative orbit trajectory pyplot.plot(relTrajectory[:,0],&#39;r&#39;,markersize=0.1, label=&quot;Along-Track&quot;) pyplot.plot(relTrajectory[:,1],&#39;g&#39;,markersize=0.1, label=&quot;Nadir&quot;) pyplot.plot(relTrajectory[:,2],&#39;b&#39;,markersize=0.1, label=&quot;Neg. Orbit Normal&quot;) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Reative Distance (km)&#39;) pyplot.legend(loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) pyplot.show() . # utl.createVTSSim(&#39;hamr&#39;, prop.sol[&#39;time&#39;], prop.sol[&#39;state&#39;][:,0:10], &#39;6DoF&#39;) utl.createVTSSim(&#39;steel&#39;, prop.sol[&#39;time&#39;], prop.sol[&#39;state&#39;][:,0:10], &#39;3DoF&#39;) .",
            "url": "https://rasitabay.github.io/blog/orbit%20propagator/hamr/6d/jupyter/2020/04/21/6D-Orbit-Propagation.html",
            "relUrl": "/orbit%20propagator/hamr/6d/jupyter/2020/04/21/6D-Orbit-Propagation.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Two-Line Element Estimation Using Machine Learning",
            "content": "1. Introduction . Two-line elements are widely used for space operations to predict the orbit with a moderate accuracy for 2-3 days. Local optimization methods, such as the nonlinear least squares method with differential corrections, can estimate a TLE as long as there exists an initial estimate that provides the desired precision. Global optimization methods to estimate TLEs are computationally intensive, and estimating a large number of them is prohibitive. In this paper, the feasibility of estimating TLEs using machine learning methods is investigated. First, a Monte-Carlo approach to estimate a TLE, when there are no initial estimates that provide the desired precision, is introduced. The proposed Monte-Carlo method is shown to estimate TLEs with root mean square errors below 1 km for space objects with varying area-to-mass ratios and orbital characteristics. Second, gradient boosting decision trees and fully-connected neural networks are trained to map orbital evolution of space objects to the associated TLEs using 8 million publicly available TLEs from the US space catalog. The desired precision in the mapping to estimate a TLE is achieved for one of the three test cases, which is a low area-to-mass ratio space object. . 2. Background . 2.1 Two-Line Elements . Two-line element sets (TLEs) and Simplified General Perturbations #4 (SGP4) are widely used for space operations to predict trajectories of space objects with moderate accuracy (tens of kilometers) for 2-3 days. SGP4 provides the reference trajectory that is updated by fitting the actual trajectory, and TLEs are the updated parameters that approximate it. SGP4 includes zonal terms up to J5 of the gravitational potential of the Earth, neutral atmosphere with exponential decay and partially modeled third-body mass interactions. Bstar ($B^{*}$) is an element of a TLE that determines the effect of air drag on the trajectories of the space objects. Table 1 shows the parameters of a TLE. However, it should be noted that TLE parameters are mean elements that are estimated by fitting the SGP4 trajectory and observed trajectory, and their values can drastically changed compared to the osculating elements at epoch. . Parameters Equations Description . $B^{*}$ | $B^{*}$=$ frac{1}{2} frac{C_{D}A}{m} rho$ | Determines the effect of air drag on the trajectories | . $n$ | $n$=$ sqrt{ frac{ mu}{a^{3}}}$ | Defines the angular frequency of the orbit (shape of the orbit) | . $i$ | - | Defines the orientation of the orbital plane with respect to the Earth | . $ Omega$ | - | Defines the orientationof an orbit with respect to the $ hat{z}$ axis, which is parallel to the rotation axis of the Earth | . $ omega$ | - | Defines the orientation of the orbital plane with respect to the Earth around the axis parallel to the angular momentum vector of the orbit | . $e$ | - | Defines the shape of the orbit | . $m$ | - | Defines the location of a space object in its orbit | . Table 1 : Parameters of a two-line element set.2.2 Two-Line Element Estimation Methods . There are a few studies that investigate different approaches to estimate TLEs. As stated above, TLEs include the mean states estimated by fitting observations to the dynamics provided by SGP4, and they can only be used with SGP4 [1]. Keplerian orbital elements are used as initial estimates of TLEs by the differential corrections and nonlinear least squares methods [2]. Kalman filter is investigated to estimate TLEs using onboard Global Positioning System (GPS) data [3]. However, the above methods, which search for the local minimum of the objective function, which is the sum of the squares of the position and velocity errors, depend on the availability of a reasonable initial estimate of a TLE. Although methods, such as genetic algorithms [4] and invasive weed optimization [5], that do not require an initial estimate of the TLEs have been investigated, they are reported to be computationally intensive as they search for the global optimum. To summarize, the two main approaches in the literature to estimate a TLE use either computationally expensive global search or local search which depends on having an initial estimate. This represents a significant shortcoming in the current state-of-the-art of orbit determination using TLEs. The present work addresses this shortcoming. . 2.3 Machine Learning . This work utilizes machine learning methods, namely the gradient boosting trees and fully-connected neural networks, to approximate the inverse mapping of publicly available SGP4 algorithm [1] for LEO objects by learning to map the orbital evolution to the associated TLE. The capability of approximating such mapping using machine learning will enable to represent time series orbital data in latent space that can be used with orbit propagators. The gradient boosting trees are machine learning models that are suitable for determining non-linear and sharp decision boundaries. The boosting is achieved by adding weak learners, such as decision trees, to determine the complex decision boundaries. The first successful application of such an idea is adaptive boosting (AdaBoost). AdaBoost generates a sequence of classifiers, and each classifier puts more weight on the samples that are not classified accurately in the previous iteration. Such an approach enables each classifier to capture different features in the input data, and more complex decision boundaries can be defined in the end. The boosting idea is further improved by defining the framework as a numerical optimization in the function space. The objective function is minimized by adding weak learners using the steepest-descent method. . Artificial neural networks are machine learning models that are suitable for determining smooth nonlinear decision boundaries. According to the universal approximation theory [6], any Borel measurable function can be approximated by a feedforward neural network as long as it includes enough hidden units [7]. In addition, a neural network may also approximate any discrete function irrespective of its dimensions given that dimension is finite. Although the universal approximation theory states that any closed and bounded continuous function can be approximated by multilayer perceptrons, the training scheme to approximate that function might fail due to overfitting or failure of the optimization algorithm [7]. . 3. Methods . 3.1 Monte-Carlo approach . A Monte-Carlo approach that searches for the global minimum of the sum of the squares of the position and velocity errors to estimate a TLE without any initial data is presented. However, the proposed method is computationally intensive. Therefore, this work also investigates the feasibility of using machine learning to predict reasonable initial estimates of TLEs that can be utilized by the proposed Monte-Carlo approach. Figure 1 outlines the proposed method that extends the differential corrections method using the Monte-Carlo technique to estimate a TLE that satisfies a given criterion. The following procedure describes how to obtain such TLEs. . . Figure 1:The Monte-Carlo approach to estimate a TLE.The inner loop leverages the differential corrections to correct the initial parameters to fit the orbit using non-linear least squares method. The differential corrections scheme iteratively improves the sampled osculating TLE to match the position vector at the epoch time. The stopping criterion is selected as $10^{-8}$ and smaller for the magnitude of position error in kilometers. Equation below shows how a Jacobian matrix, which is computed by the finite-difference method, relates the change in the TLE to the change in the position and velocity vectors at the epoch. . begin{equation} label{eq:diffCorrection} begin{split} delta tle_{epoch} &amp;= left( frac{ partial f_{sgp4}}{ partial tle_{epoch}} right)^{-1} delta rv_{epoch} &amp;= (A_{t_{0}})^{-1} delta b_{t_{0}} end{split} end{equation}where $tle_{epoch}$ is the TLE parameters at the epoch, $f_{sgp4}$ is SGP4, $rv_{epoch}$ are the propagated position and velocity vectors at the epoch, $A_{t_{0}}$ is the Jacobian, and $ delta b_{t_{0}}$ is the residual at the epoch. Equation below shows the normal equation that relates all of the changes in the position and velocity vectors associated with the observational data available to the change in the TLE at epoch time [2]. In this work, all observational data are assumed to have the same weight, therefore, $W$ is assumed to be the identity matrix. Equation above is used for differential corrections in the inner loop and Equation below is used for differential corrections with nonlinear least squares method in the outer loop (Figure 1). . begin{equation} label{eq:diffCorrectionNonlinear} begin{split} delta tle_{epoch} &amp;= (A^{T}WA)^{-1}A^{T}Wb &amp;= left( sum_{i}^{N} A_{(t_{i})}^{T}A_{(t_{i})} right)^{-1} sum_{i}^{N} A_{(t_{i})}^{T}b_{(t_{i})} end{split} end{equation} noindent where $tle_{epoch}$ is the TLE parameters at the epoch, $f_{sgp4}$ is SGP4, $rv_{epoch}$ are the propagated position and velocity vectors at any time, $A_{t_{i}}$ is the Jacobian, and $ delta b_{t_{i}}$ is the residual at any time. . 3.2 Machine Learning Assisted Estimation . 3.2.1 Feature Selection . In the present work, Equinoctial orbital elements and a parameter that we shall call instance (Table 2) that defines the sequence of the data with fixed time interval are selected as feature vectors (input data for the machine learning models). The above orbital elements are non-singular, and the magnitude of their values are bounded, thus well-suited for the training process. The Keplerian mean motion, in radians per hour, replaces semi-major axis of the Equinoctial elements. The feature vectors are generated from 8,206,374 TLEs which are all LEO objects that have semi-major axes of 8,378 km and smaller in the space catalog. TLEs are restricted to the LEO orbital regime to ensure that all space objects are subject to a significant orbital perturbation due to atmospheric drag. . . Figure 2: Feature vectors selected for the machine learning models. Parameters Equations . Instance | [2.452, 2.402, ..., 0.049, 0.0] | . Mean motion (rad/h) | $n = sqrt{ frac{ mu}{a^{3}}}$ | . Ex | $E_{x} = ecos( omega+ Omega)$ | . Ey | $E_{y} = esin( omega+ Omega)$ | . Hx | $H_{x} = tan( frac{i}{2})cos( Omega)$ | . Hy | $H_{y} = tan( frac{i}{2})sin( Omega)$ | . Lv | $Lv = nu + omega + Omega$ | . Table 2 : Feature vectors selected for the machine learning models.3.2.2 Boosting Trees . An optimized distributed gradient boosting library called XGBoost (version 0.7) is used for this work (Available at https://github.com/dmlc/xgboost). A gradient boosting tree model is trained for each element of a TLE, namely Bstar ($B^{*}$), eccentricity ($e$), inclination ($i$), mean anomaly ($m$), mean motion ($n$), right ascension of the ascending node ($ Omega$), and the argument of perigee ($ omega$). The construction of individual gradient boosting trees for each TLE parameter is chosen because they cannot map to a multidimensional output for regression problems due to the architecture of the decision trees, whereas neural networks can. . Table 3 shows the gradient boosting tree hyperparameters that are optimized for training XGBoost models. The learning rate controls the step size for the gradient-based optimizer. The number of estimators controls the number of trees. The maximum depth limits the depth of each tree which is used to increase the complexity of the model. The minimum child weight is required to avoid overfitting of the training data. A node is split as long as the resultant split leads to a positive reduction in the loss function (mean square error in this work). Gamma can control the split of nodes. The rest of the parameters in Table 3 are used to avoid overfitting by forcing the model to be less complex. The hyperparameters of the gradient boosting trees are tuned empirically by considering the bias-variance tradeoff to ensure the trained models have generalization capability (Table 4). The grid search methods are not preferred because the models are trained with large amount of data. . Parameters $B^{*}$ $i$ $ Omega$ $e$ $ omega$ $m$ $n$ . Learning rate | 0.1 | 0.3 | 0.3 | 0.3 | 0.2 | 0.2 | 0.1 | . No. of estimators | 100 | 80 | 70 | 70 | 80 | 80 | 100 | . Maximum depth | 20 | 20 | 20 | 20 | 25 | 25 | 25 | . Min. child weight | 7 | 9 | 11 | 13 | 27 | 27 | 1 | . Gamma | - | - | - | - | 0.7 | 1.0 | - | . Subsample | 0.7 | 0.7 | 0.6 | 0.7 | 0.1 | 0.2 | 1.0 | . Col. sample by tree | 0.7 | 0.8 | 0.7 | 0.7 | 0.2 | 0.3 | 1.0 | . Col. sample by level | 0.7 | 0.8 | 0.7 | 0.7 | 0.2 | 0.3 | 1.0 | . Alpha | 0.2 | 0.2 | 0.2 | 0.1 | 0.9 | 0.7 | 0.0 | . Table 3 : Hyperparameters of XGBoost. Models Training Squared Mean Error Test Squared Mean Error . $B^{*}$ | 3.9e-5 | 6.3e-5 | . $i$ | 7.1e-6 | 2.1e-5 | . $ Omega$ | 3.5e-4 | 2.9e-3 | . $e$ | 1.6e-6 | 1.3e-5 | . $ omega$ | 0.21 | 0.32 | . $m$ | 0.15 | 0.26 | . $n$ | 9.8e-9 | 2.2e-8 | . Table 4 : Bias-Variance trade-off for XGBoost.3.2.3 Deep Learning . Since neural networks can be trained with additional data without being trained from scratch, they are more versatile compared to other methods such as decision trees. In this paper, a fully-connected neural network architecture (Keras-version 2.2.0 using Tensorflow backend-version 1.8.0) is chosen to approximate the TLE estimation scheme (Figure 3). It should be noted that the inputs are standardized. . . Figure 3: Fully-connected neural network architecture used for the work.Table 5 shows the hyperparameters of the fully-connected neural network that are optimized for neural network models. The Adam optimizer is chosen due to its computational efficiency for problems with large amount of data, and it computes the moving average of the gradients and squared gradients. The parameters of the Adam optimizer that control the decay rate of the moving averages {KA2015} are $ beta_{1}$ and $ beta_{2}$. The learning rate controls the step size for the Adam optimizer while the decay parameter decays the learning rate at each epoch. The hyperparameters of the fully-connected neural networks are tuned empirically by considering the bias-variance tradeoff to ensure the trained models have generalization capability (Table 6). The grid search methods are not preferred because the models are trained with large amount of data. . Parameters $B^{*}$ $i$ $ Omega$ $e$ $ omega$ $m$ $n$ . Learning rate | 9e-5 | 9e-5 | 9e-5 | 9e-5 | 9e-5 | 9e-5 | 1e-5 | . Optimizer | Adam | Adam | Adam | Adam | Adam | Adam | Adam | . $ beta_{1}$ | 9e-4 | 9e-4 | 9e-4 | 9e-4 | 9e-4 | 9e-4 | 9e-4 | . $ beta_{2}$ | 999e-6 | 999e-6 | 999e-6 | 999e-6 | 999e-6 | 999e-6 | 999e-6 | . Decay | 1e-07 | 1e-07 | 1e-07 | 1e-07 | 1e-07 | 1e-07 | 1e-07 | . Epoch number | 50 | 50 | 50 | 50 | 50 | 50 | 50 | . Table 5 : Hyperparameters of FC network. Models Training Squared Mean Error Test Squared Mean Error . $B^{*}$ | 1.4e-6 | 2.4e-6 | . $i$ | 1.3e-4 | 9.8e-5 | . $ Omega$ | 0.02 | 0.01 | . $e$ | 1.8e-7 | 2e-7 | . $ omega$ | 0.24 | 0.22 | . $m$ | 0.2 | 0.3 | . $n$ | 4.6e-9 | 5.3e-9 | . Table 6 : Bias-Variance trade-off for the FC network. 4. Results . 4.1 Monte-Carlo Approach . A novel approach to estimate a TLE from a given ephemeris without any initial estimate of the TLE using a Monte-Carlo method is introduced. Three particular cases with varying area-to-mass ratios, orbital characteristics and solar activity phases, where the standard differential corrections with the nonlinear least squares method {VC2008} cannot generate a valid TLE, are chosen as test cases. Initial orbital parameters for the test cases are propagated forward in time using a three-degree-of-freedom (3-DOF) numerical orbit propagator over 1 day. The orbital evolution is defined in the Vehicle Velocity, Local Horizontal (VVLH) reference frame (Figure 4).The perturbations included are: air drag (Harris-Priester model), the Earth&#39;s aspherical gravitational potential with order and degree 20 (Holmes-Featherstone model), solar radiation pressure (Lambertian sphere model), and Sun and Moon third-body gravitational perturbations. Parameters of the test cases used for the numerical propagation are given in Table 7. . . Figure 4: The Vehicle Velocity, Local Horizontal (VVLH) reference frame. Parameters Test case #1 Test case #2 Test case #3 . Epoch (UTC) | 2018-12-12T18:47 | 2014-01-15T10:37 | 2009-08-03T15:18 | . Period (min) | $99.71$ | $94.65$ | $97.32$ | . Mass (kg) | $800$ | $3$ | $1$ | . Area ($m^{2}$) | $6$ | $0.03$ | $0.1$ | . $n$ ($ frac{rad}{s}$) | $0.00105$ | $0.001106$ | $0.001076$ | . $i$ (deg) | $101.45$ | $85.12$ | $97.60$ | . $ Omega$ (deg) | $53.03$ | $90.16$ | $253.29$ | . $ omega$ (deg) | $9.33$ | $204.08$ | $235.32$ | . $m$ (deg) | $359.44$ | $196.9$ | $178.81$ | . $e$ | $0.022353$ | $0.011556$ | $0.025822$ | . $C_{D}$ | $2.2$ | $1.5$ | $0.8$ | . Table 7 : The parameters of the test cases used for the numerical propagator.Figure 4 shows the absolute relative distance between the numerically propagated orbit and the SGP4propagated orbit when the inner loop is satisfied for the test cases. . . Figure 4: the absolute relative distances with respect to numerically propagated orbit when Keplerian elements are used as TLEs.Figure 5 shows the absolute relative distance between the numerically propagated orbit and the SGP4propagated orbit when the inner loop is satisfied for the test cases. . . Figure 5: Absolute relative distance between the numerically propagated orbit and the SGP4propagated orbit when the inner loop is satisfied.Figure 6 shows the absolute relative distance with respect to numerically propagated orbit when Kep-lerian elements are used as TLE for the test cases. . . Figure 6: Absolute relative distance with respect to numerically propagated orbit when Kep-lerian elements are used as TLE.In conclusion, the present Monte-Carlo method can estimate a TLE without any initial estimate. Such capability is beneficial to space situational awareness (SSA) because TLEs are required for establishing first contact with the spacecraft during the Launch and Early Orbit Phase (LEOP). Most ground stations have propriety software that requires TLEs to track the satellites. Although the proposed method addresses the difficulty related to unavailability of the initial estimate, the inner loop is iterated 50 times on the average to find an initial estimate with desired precision. Therefore, the feasibility of using machine learning methods to generate initial estimates with the desired precision is investigated. . 4.2 Machine Learning Assisted TLE Estimation . The performances of machine learning methods differ based on the features selected. Two different machine learning models are trained to predict each mean elements of a TLE, and the best performing model is chosen to be validated by the errors in the orbital evolution. The error in the orbital evolution is computed by propagating TLEs estimated by the selected best machine learning models and the associated CSpOC TLEs backward in time using SGP4 for 50 orbital periods. Table 8 summarizes the best performing models for each mean element that are discussed in detail above. . Mean elements Best machine learning models . $B^{*}$ | Gradient boosting trees | . $n$ | Fully-connected neural network (unless the absolute residual between two models are larger than 0.0002 rad/min for values of $n$ between 0.053 rad/min and 0.067 rad/min) | . $i$ | Gradient boosting trees (unless the absolute residual between two models are larger than 0.014 rad) | . $ Omega$ | Gradient boosting trees | . $ omega$ | Gradient boosting trees | . $m$ | Fully-connected neural network | . $e$ | Fully-connected neural network | . Table 8 : Best machine learning models for predicting mean elements of a TLE.Figure 7 shows the absolute relative distances with respect to numerically propagated orbits when TLEs estimated by machine learning models are used for the test cases. . . Figure 7: Absolute relative distances with respect to numerically propagated orbits when TLEs estimated by machine learning models are used.Figure 8 shows the absolute relative distances between the numerically propagated orbits and the SGP4 propagated orbits when the initial estimates that are computed by the machine learning models are improved by nonlinear least squares with differential corrections method for the test cases. . . Figure 8: Absolute relative distances between the numerically propagated orbits and the SGP4 propagated orbits when the initial estimates that are computed by the machine learning models are improved by nonlinear least squares with differential corrections method.In conclusion, the present best machine learning models (selected based ontheir performance) that are trained with TLEs obtained from the official US Space Catalog have the potential to provide an initial estimate to estimate a TLE with desired precision. The inner loop, which searches for the global minimum by using brute force approach, in the Monte-Carlo TLE estimation method can be replaced by machine learning models because there isno secular error growth in the orbital evolution of TLEs predicted by machine learning models (test case #1). Therefore, the standard differential corrections with nonlinear least squares method can converge to a local minimum, and the computation time can be reduced significantly. . 5. References . [1] Vallado, D., Crawford, P., Hujsak, R. and Kelso, T.S., 2006, August. Revisiting spacetrack report# 3. In AIAA/AAS Astrodynamics Specialist Conference and Exhibit (p. 6753). . [2] Vallado, D. and Crawford, P., 2008, August. SGP4 orbit determination. In AIAA/AAS Astrodynamics Specialist Conference and Exhibit (p. 6770). . [3] Montenbruck, O. and Gill, E., 2000, June. Real-Time estimation of SGP4 orbital elements from GPS navigation data. In International Symposium Space Flight Dynamics (pp. 26-30). Biarritz, France. . [4] Goh, S.T. and Low, K.S., 2018, March. Real-time estimation of satellite&#39;s two-line elements via positioning data. In 2018 IEEE Aerospace Conference (pp. 1-7). IEEE. . [5] Bolandi, H., Ashtari Larki, M.H., Sedighy, S.H., Zeighami, M.S. and Esmailzadeh, M., 2015. Estimation of Simplified General Perturbations model 4 orbital elements from global positioning system data by invasive weed optimization algorithm. Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering, 229(8), pp.1384-1394. . [6] Cybenko, G., 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4), pp.303-314. . [7] Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press. . [8] Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. . 6. Code . Since the codes used to generate this research includes different components, codes are not shared in the blog post. Please contact me for the source code. .",
            "url": "https://rasitabay.github.io/blog/two-line%20elements/mean%20elements/tle/estimation/machine%20learning/2020/04/20/two-line-elemets-machine-learning.html",
            "relUrl": "/two-line%20elements/mean%20elements/tle/estimation/machine%20learning/2020/04/20/two-line-elemets-machine-learning.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rasitabay.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rasitabay.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am applying machine learning to the challenges associated with Space Situational Awareness (SSA) ranging from sensor fusion to collision avoidance and optimal maneuvers. I build machine learning models that tackle various practical challenges. I am experienced in deploying optimized machine learning models for Low-SWaP devices. . Rasit Abay . PhD Candidate . UNSW Canberra .",
          "url": "https://rasitabay.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rasitabay.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}