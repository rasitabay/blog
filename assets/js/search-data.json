{
  
    
        "post0": {
            "title": "Predicting Collision Risk from Historical Collision Data Messages (CDMs) using Machine Learning",
            "content": "1. Introduction . The number of close approaches between space objects is increasing due to the proliferation of satellites and debris in orbit. It is a routine operation for satellite operators to avoid probable collisions by conducting maneuvers based on conjunction data messages (CDMs). The number of CDMs issued weekly are a couple of orders of magnitude larger than the actionable ones, and this puts pressure on satellite operators. Therefore, an automated process that can predict collision risk with desired accuracy a couple of days ahead to allow satellite operators to plan for collision avoidance maneuvers is desired. In this work, the feasibility of leveraging deep learning to predict the final collision risk between two space objects from the history of CDMs is investigated. The CDMs issued for the satellites that ESA&#39;s Space Debris Office supports are used to train an ensemble of Manhattan-LSTM models. The proposed ensemble model leverages the similarity scores between the input pairs instead of determining the decision boundary between high and low-risk close approach classes to address the imbalanced data. The significant imbalance and class overlapping in the data is also addressed by casting the problem as an anomaly detection problem and utilizing ensemble of various models. The proposed ensemble model with majority vote can predict collision risks two days ahead with a precision of 91% for high-risk cases (after adding up the predictions of two-stages). It should ne noted that the solution is two-stage. First stage involves feature engineering and casting the problem as anomaly detection problem by branching normal and anomalous cases with the most important feature, and that gives the precision score of 75% for high-risk cases. The second stage leverages the Manhattan-LSTM to decide whether each input pairs are similar, and this gives f1 score of 80% for anomaly detection of high-risk cases. Therefore, it is possible to determine low-risk to high-risk at close approach time anomalies. . 2. Data Preparation . Machine learning models are sensitive to the training data, and the attributes of the data should be understood before training models with them. The dataset used in this work includes time-series of CDMs issued for close approaches between a satellite and a space object. However, each example includes some information that exists in the standard CDM format, yet not all. The training dataset has 162634 rows (CDMs), 13154 of which are unique close approaches (approximately 12 CDMs for each event), and 103 columns(features from each CDM). Some features from CDMs are probability of collision risk, miss distance, relative position and velocity, solar activity indices,uncertainty in position and velocity, radar cross sections, and some orbitaldata about both space object. Further details about all features available in the dataset can be accessed on Kelvins platform [1]. Since the training dataset is small, 20 most informative features are used for training models to makesure the model doesn’t learn the noise in the data. . The training dataset has time-series of CDMs ranging from 7 days to a couple hours before the close approach. ESA Space Debris Office notifies flight teams to plan for possible maneuvers 2 days before the time of the closest approaches for their satellites, and they conduct futher analysis to decide whether to maneuver within 1 day. Since there exists uncertainty in orbit determination and propagation of space objects, it is preffered to start planning for collision avoidance maneuvers a couple of days ahead of the close approach time to keep uncertainty small and let flight teams have enough time to analyse the maneuvers to be conducted. . Since it is desired to predict the collision risk two or more days ago, the latest available collision risk value (target label) and a CDM (input features) which is issued two or more days before the close approach should be selected for each close approach event. Figure 1 shows how some input features of a CDM and a collision risk value as a target label (log base 10) are selected as described above to form a candidate input-output pair (most informative features are selected during feature engineering) for training machine learning models from time-series of CDMs. . . Figure 1: The selection of input features and the associated collision risk value as target (log base 10).Due to my domain expertise, I directly investigated whether the latest available collision risk values that are issued 2 or more days before the close approach are enough to predict the target risk class accurately, and I realised this was the case. Collision risk is derived from other input features, such as miss distance, position covariance and physical sizes of space objects, and it is an important feature. However, there are 23 cases that are turn out to be high-risk and 269 cases that turn out to be low-risk when the latest available collision risk values predict low-risk and high-risk respectively. Therefore, this work casts the collision risk prediction problem as an anomaly detection problem. It investigates the feasibility of using machine learning to predict collision risk class (high or low-risk) with desired accuracy by detecting anomalies (Figure 2). . . Figure 2: The outline of sample sizes when the normal and anomaly groups are splitted by the latest available collision risk value.The dataset has many challenges due to its small size, and it is more sensitive to the quality of data in general. Imbalance ratio, which is the ratio of the majority to the minority class, is 370 for low-risk target class and 3 for high-risk target class. Anomalies for low-risk target class has only 23 examples which is a challenge to even split data for validation purposes because the validation dataset should have enough examples for results to be statistically meaningful. In addition, the total number of samples for high-risk target class is 339. The stratified 3-folds cross-validator is implemented to avoid overfitting. An ensemble of various simple models with different hyperparameters that determines the similarity in input pairs is leveraged for better generalization. Figure shows the 2-D feature space representation of some input features (features that fuse multiple other features), namely time to close approach, mahalanobis distance, maximum risk estimate and scaling, and target labels using t-distributed stochastic neighbor embedding (t-SNE) algorithm [2]. It should be noted that due to significant class imbalance and class overlapping high-risk to low-risk anomalies, no statistical model can be developed for that case. . . Figure 3: T-distributed stochastic neighbor embedding representation of low-risk (left) and high-risk (right) target classes. 3. Manhattan-LSTM Model . There are two neural networks in Manhattan-LSTM model, and they share weights [4]. The model utilizes LSTM to read the features that are derived from CDMs. Then, the similarity score between the input pairs is computed by using the similarity function $g = exp(-|h_{a}-h_{b}|_{1})$ (Figure 4). The LSTM is chosen because it allows variable lengths (to allow missing data to be included) as inputs, and this is not because the inputs are time-series. The total number of pairs for training is 1 million (normal-abnormal), and 10000 pairs are prepared for validation. . . Figure 4: The outline of Manhattan-LSTM model.Table 1 shows the features that are selected based on the distribution differences between normal and anomalous case. . Features Variable Description . Time to tca | time_to_tca | Time interval between CDM creation and time-of-closest approach (days) | . Max risk estimate | max_risk_estimate | Maximum collision probability obtained by scaling combined covariance | . Mahalonobis distance | mahalanobis_distance | Miss distance scaled with uncertainty | . Miss distance | miss_distance | Relative position between chaser &amp; target at tca (m) | . Number of CDMs issued | no_larger_2 | Number of CDMs issued before 2 days to the tca | . Mean of risk values | mean_larger_2 | Mean of collsion risk values for the CDMs issued before 2 days to the tca | . STD of risk values | std_larger_2 | STD of collsion risk values for the CDMs issued before 2 days to the tca | . Debris positional uncertainty (cross-track) | c_sigma_n | Covariance (cross-track) position standard deviation (sigma) in meters | . Debris positional uncertainty (along-track) | c_sigma_t | Covariance transverse (along-track) position standard deviation (sigma) in meters | . Debris positional uncertainty (radial) | c_sigma_r | Covariance radial position standard deviation (sigma) in meters | . Debris positional covariance determinant | c_position_covariance_det | Determinant of covariance | . Debris number of observations used | c_obs_used | Number of observations used for orbit determination (per CDM) | . Table 1 : Features selected as inputs for Manhattan-LSTM.Table 2 shows the hyperparameters that are used to train the Manhattan-LSTM. . Hyperparameters Values . Hidden layer size | 32 | . Batch size | 64 | . Epoch number | 100 | . Activation function | tanh | . Dropout | 0.05 | . Learning rate | 2e-5 | . Table 2 : The hyperparameters that are used to train the Manhattan-LSTM. 4. Results . Figure 5 shows the loss values for both training and validation data, and Figure 5 shows the area-under-curve values during training. . . Figure 5: Loss values for both training and validation data during training. . Figure 6: AUC values for both training and validation data during training.In Figure 7, correlation matrix for the test data is presented using the model for the epoch 55, which is the epoch when overfitting starts. . . Figure 6: AUC values for both training and validation data during training. 5. References . [1] https://kelvins.esa.int/collision-avoidance-challenge/ . [2] Maaten, L.V.D. and Hinton, G., 2008. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), pp.2579-2605. http://www.jmlr.org/papers/v9/vandermaaten08a.html . [3] Mueller, J. and Thyagarajan, A., 2016, March. Siamese recurrent architectures for learning sentence similarity. In thirtieth AAAI conference on artificial intelligence. https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewPaper/12195 . 6. Code . # Load the required libraries from sklearn.metrics import fbeta_score import numpy as np from matplotlib import pyplot as plt import pandas as pd import seaborn as sns import os import random from sklearn.preprocessing import LabelEncoder from sklearn.metrics import mean_squared_error from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import KFold from sklearn.manifold import TSNE from sklearn.decomposition import PCA from time import time from sklearn.model_selection import train_test_split import itertools import datetime from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda, Dense, Conv1D, Flatten, Dropout import tensorflow.keras.backend as K from tensorflow.keras.optimizers import Adadelta, Adam, SGD from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.metrics import AUC from sklearn.manifold import TSNE from datagen import prepare_data pd.set_option(&#39;display.max_columns&#39;, None) . #prepare the data to test challenge metric #define paths path_to_dataset = &#39;dataset/&#39; #read the training data train = pd.read_csv(os.path.join(path_to_dataset,&#39;train_data.csv&#39;)) test = pd.read_csv(os.path.join(path_to_dataset,&#39;test_data.csv&#39;)) #Shapes of the train and test dataset print(&quot;shape of train data {}&quot;.format(train.shape)) print(&quot;shape of test data {}&quot;.format(test.shape)) . #train time to close approach min and max train[&quot;time_to_tca&quot;].describe().loc[[&quot;min&quot;,&quot;max&quot;]] . #train time to close approach min and max test[&quot;time_to_tca&quot;].describe().loc[[&quot;min&quot;,&quot;max&quot;]] . print(&quot;number of events that has less than 2.0 days data : &quot;,len(train[train[&quot;time_to_tca&quot;]&lt;2.0].groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) print(&quot;number of events that has more than 2.0 days data : &quot;,len(train[train[&quot;time_to_tca&quot;]&gt;2.0].groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) print(&quot;total number of events : &quot;,len(train.groupby(&quot;event_id&quot;).count()[&quot;time_to_tca&quot;].tolist())) events_less_than_two_days = train[train[&quot;time_to_tca&quot;]&lt;2.0].groupby(&quot;event_id&quot;)[&quot;event_id&quot;].first().tolist() events_more_than_two_days = train[train[&quot;time_to_tca&quot;]&gt;2.0].groupby(&quot;event_id&quot;)[&quot;event_id&quot;].first().tolist() . # number of events that has time_to_tca larger and smaller than 2 at the same time events_trainable = [value for value in events_less_than_two_days if value in events_more_than_two_days] print(&quot;number of trainable events : &quot;,len(events_trainable)) . #generate new training data that uses last available CDM (closest to 2 days) and adds latest CDM risk as target variable lenData = len(events_trainable) new_train = [] target_variable = [] no_larger_2 = [] mean_larger_2 =[] std_larger_2 = [] mean_nan_number =[] std_nan_number = [] for cnt in range(lenData): new_train.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)][-1:].values.tolist()[0]) target_variable.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])][-1:].risk.values[0]) no_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].shape[0]) mean_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].risk.mean()) std_larger_2.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].risk.std(ddof=0)) mean_nan_number.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].isnull().sum(axis=1).mean()) std_nan_number.append(train[(train[&quot;event_id&quot;]==events_trainable[cnt])&amp;(train[&quot;time_to_tca&quot;]&gt;=2.0)].isnull().sum(axis=1).std(ddof=0)) new_traintrain_pd = pd.DataFrame(new_train,columns=train.columns.tolist()) #add target value to the dataset target_variable_pd = pd.DataFrame(target_variable,columns=[&quot;target_risk&quot;]) no_larger_2_pd = pd.DataFrame(no_larger_2,columns=[&quot;no_larger_2&quot;]) mean_larger_2_pd = pd.DataFrame(mean_larger_2,columns=[&quot;mean_larger_2&quot;]) std_larger_2_pd = pd.DataFrame(std_larger_2,columns=[&quot;std_larger_2&quot;]) mean_nan_number_pd = pd.DataFrame(mean_nan_number,columns=[&quot;mean_nan_number&quot;]) std_nan_number_pd = pd.DataFrame(std_nan_number,columns=[&quot;std_nan_number&quot;]) new_traintrain_pd[&quot;target_risk&quot;] = target_variable_pd.values new_traintrain_pd[&quot;no_larger_2&quot;] = no_larger_2_pd.values new_traintrain_pd[&quot;mean_larger_2&quot;] = mean_larger_2_pd.values new_traintrain_pd[&quot;std_larger_2&quot;] = std_larger_2_pd.values new_traintrain_pd[&quot;mean_nan_number&quot;] = mean_nan_number_pd.values new_traintrain_pd[&quot;std_nan_number&quot;] = std_nan_number_pd.values new_traintrain_pd.head(5) . # Determines the high risk events new_traintrain_pd[&#39;target_risk_class&#39;]=np.nan new_traintrain_pd.loc[new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0,&#39;target_risk_class&#39;] = int(1) new_traintrain_pd.loc[new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0,&#39;target_risk_class&#39;] = int(0) print(&quot;number of high risk events&quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==1].shape[0]) print(&quot;number of low risk events&quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==0].shape[0]) print(&quot;ratio of high risk versus low risk &quot;,new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==1].shape[0]/new_traintrain_pd[new_traintrain_pd[&#39;target_risk_class&#39;]==0].shape[0]*100) . # Uses the latest available risk value to determine the normal and anomalous cases new_traintrain_pd[&#39;predicted_risk&#39;] = 0 new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;]&lt;-6.0,&#39;predicted_risk&#39;] = -6.000000000000001 new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;]&gt;=-6.0,&#39;predicted_risk&#39;] = new_traintrain_pd.loc[new_traintrain_pd[&#39;risk&#39;] &gt;=-6.0].risk.values . # Assigns the classes based on risk values new_traintrain_pd[&#39;predicted_risk_class&#39;]=np.nan new_traintrain_pd.loc[new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0,&#39;predicted_risk_class&#39;] = int(1) new_traintrain_pd.loc[new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0,&#39;predicted_risk_class&#39;] = int(0) print(&quot;number of high risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==1].shape[0]) print(&quot;number of low risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==0].shape[0]) print(&quot;ratio of high risk versus low risk with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==1].shape[0]/new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_class&#39;]==0].shape[0]*100) . new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]=np.nan new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0),&#39;predicted_risk_anomaly&#39;] = int(0) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0),&#39;predicted_risk_anomaly&#39;] = int(1) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&gt;=-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&lt;-6.0),&#39;predicted_risk_anomaly&#39;] = int(2) new_traintrain_pd.loc[(new_traintrain_pd[&#39;predicted_risk&#39;]&lt;-6.0)&amp;(new_traintrain_pd[&#39;target_risk&#39;]&gt;=-6.0),&#39;predicted_risk_anomaly&#39;] = int(3) print(&quot;number of 0 to 1 risk anomaly events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==3].shape[0]) print(&quot;number of 1 to 0 risk anomaly events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==2].shape[0]) print(&quot;number of 0 to 0 risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==1].shape[0]) print(&quot;number of 1 to 1 risk events with above criterion&quot;,new_traintrain_pd[new_traintrain_pd[&#39;predicted_risk_anomaly&#39;]==0].shape[0]) . #convert categorical c_object_type le = LabelEncoder() new_traintrain_pd[&quot;c_object_type&quot;] = le.fit_transform(new_traintrain_pd[&quot;c_object_type&quot;]) . # Computes the missing value percentage obj = new_traintrain_pd.isnull().sum() for key,value in obj.iteritems(): if value!=0: print(&quot;key : {} -&gt; missing data (percentage) :{} %&quot;.format(key, value/8892.0*100)) . # Fills missing values with mean values and checks whether there exist more missing data new_traintrain_pd.fillna(new_traintrain_pd.mean(), inplace=True) obj = new_traintrain_pd.isnull().sum() for key,value in obj.iteritems(): if value!=0: print(&quot;key : {} -&gt; missing data (percentage) :{} %&quot;.format(key, value/8892.0*100)) . # Scales the input data for standard scaler from sklearn.preprocessing import StandardScaler qt_risk = StandardScaler() new_traintrain_pd[&#39;qt_risk&#39;] = qt_risk.fit_transform(np.expand_dims(new_traintrain_pd[&#39;risk&#39;].values,axis=1)) # new_test_pd[&#39;qt_risk&#39;] = qt_risk.transform(np.expand_dims(new_test_pd[&#39;risk&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_risk&#39;]) qt_time_to_tca = StandardScaler() new_traintrain_pd[&#39;qt_time_to_tca&#39;] = qt_time_to_tca.fit_transform(np.expand_dims(new_traintrain_pd[&#39;time_to_tca&#39;].values,axis=1)) # new_test_pd[&#39;qt_time_to_tca&#39;] = qt_time_to_tca.transform(np.expand_dims(new_test_pd[&#39;time_to_tca&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_time_to_tca&#39;]) qt_max_risk_estimate = StandardScaler() new_traintrain_pd[&#39;qt_max_risk_estimate&#39;] = qt_max_risk_estimate.fit_transform(np.expand_dims(new_traintrain_pd[&#39;max_risk_estimate&#39;].values,axis=1)) # new_test_pd[&#39;qt_max_risk_estimate&#39;] = qt_max_risk_estimate.transform(np.expand_dims(new_test_pd[&#39;max_risk_estimate&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_max_risk_estimate&#39;]) qt_max_risk_scaling = StandardScaler() new_traintrain_pd[&#39;qt_max_risk_scaling&#39;] = qt_max_risk_scaling.fit_transform(np.expand_dims(new_traintrain_pd[&#39;max_risk_scaling&#39;].values,axis=1)) # new_test_pd[&#39;qt_max_risk_scaling&#39;] = qt_max_risk_scaling.transform(np.expand_dims(new_test_pd[&#39;max_risk_scaling&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_max_risk_scaling&#39;]) qt_mahalanobis_distance = StandardScaler() new_traintrain_pd[&#39;qt_mahalanobis_distance&#39;] = qt_mahalanobis_distance.fit_transform(np.expand_dims(new_traintrain_pd[&#39;mahalanobis_distance&#39;].values,axis=1)) # new_test_pd[&#39;qt_mahalanobis_distance&#39;] = qt_mahalanobis_distance.transform(np.expand_dims(new_test_pd[&#39;mahalanobis_distance&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_mahalanobis_distance&#39;]) qt_miss_distance = StandardScaler() new_traintrain_pd[&#39;qt_miss_distance&#39;] = qt_miss_distance.fit_transform(np.expand_dims(new_traintrain_pd[&#39;miss_distance&#39;].values,axis=1)) # new_test_pd[&#39;qt_miss_distance&#39;] = qt_miss_distance.transform(np.expand_dims(new_test_pd[&#39;miss_distance&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_miss_distance&#39;]) qt_no_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_no_larger_2&#39;] = qt_no_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;no_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_no_larger_2&#39;] = qt_no_larger_2.transform(np.expand_dims(new_test_pd[&#39;no_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_no_larger_2&#39;]) qt_mean_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_mean_larger_2&#39;] = qt_mean_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;mean_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_mean_larger_2&#39;] = qt_mean_larger_2.transform(np.expand_dims(new_test_pd[&#39;mean_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_mean_larger_2&#39;]) qt_std_larger_2 = StandardScaler() new_traintrain_pd[&#39;qt_std_larger_2&#39;] = qt_std_larger_2.fit_transform(np.expand_dims(new_traintrain_pd[&#39;std_larger_2&#39;].values,axis=1)) # new_test_pd[&#39;qt_std_larger_2&#39;] = qt_std_larger_2.transform(np.expand_dims(new_test_pd[&#39;std_larger_2&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_std_larger_2&#39;]) qt_c_sigma_n = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_n&#39;] = qt_c_sigma_n.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_n&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_n&#39;] = qt_c_sigma_n.transform(np.expand_dims(new_test_pd[&#39;c_sigma_n&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_n&#39;]) qt_c_sigma_t = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_t&#39;] = qt_c_sigma_t.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_t&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_t&#39;] = qt_c_sigma_t.transform(np.expand_dims(new_test_pd[&#39;c_sigma_t&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_t&#39;]) qt_c_sigma_r = StandardScaler() new_traintrain_pd[&#39;qt_c_sigma_r&#39;] = qt_c_sigma_r.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_sigma_r&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_sigma_r&#39;] = qt_c_sigma_r.transform(np.expand_dims(new_test_pd[&#39;c_sigma_r&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_sigma_r&#39;]) qt_c_position_covariance_det = StandardScaler() new_traintrain_pd[&#39;qt_c_position_covariance_det&#39;] = qt_c_position_covariance_det.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_position_covariance_det&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_position_covariance_det&#39;] = qt_c_position_covariance_det.transform(np.expand_dims(new_test_pd[&#39;c_position_covariance_det&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_position_covariance_det&#39;]) qt_c_obs_used = StandardScaler() new_traintrain_pd[&#39;qt_c_obs_used&#39;] = qt_c_obs_used.fit_transform(np.expand_dims(new_traintrain_pd[&#39;c_obs_used&#39;].values,axis=1)) # new_test_pd[&#39;qt_c_obs_used&#39;] = qt_c_obs_used.transform(np.expand_dims(new_test_pd[&#39;c_obs_used&#39;].values,axis=1)) # sns.distplot(new_traintrain_pd[&#39;qt_c_obs_used&#39;]) . # Prepares the data for Manhattan-LSTM X_train, Y_train, X_train_val, Y_train_val, new_data_pd = prepare_data(new_traintrain_pd) . # Model variables n_hidden = 32 gradient_clipping_norm = 0.5 batch_size = 64 n_epoch = 100 def exponent_neg_manhattan_distance(left, right): &#39;&#39;&#39; Helper function for the similarity estimate of the LSTMs outputs&#39;&#39;&#39; return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)) left_input = Input(shape=(len(new_data_pd.feature1[0]),1), dtype=&#39;float32&#39;) right_input = Input(shape=(len(new_data_pd.feature1[0]),1), dtype=&#39;float32&#39;) print(left_input.shape) # Since this is a siamese network, both sides share the same LSTM shared_lstm = LSTM(n_hidden,activation=&#39;tanh&#39;,dropout=0.05) left_output = shared_lstm(left_input) right_output = shared_lstm(right_input) # Calculates the distance as defined by the MaLSTM model malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output]) # Pack it all up into a model malstm = Model([left_input, right_input], [malstm_distance]) optimizer = Adam(learning_rate=0.00002,decay=0.000002) malstm.compile(loss=&#39;mean_squared_error&#39;, optimizer=optimizer, metrics=[AUC()]) # Start training training_start_time = time() # Include the epoch in the file name (uses `str.format`) checkpoint_path = &quot;cp_epoch_{epoch:04d}.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model&#39;s weights every 5 epochs cp_callback = ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5) malstm_trained = malstm.fit([X_train[&#39;left&#39;], X_train[&#39;right&#39;]], Y_train, batch_size=batch_size, nb_epoch=n_epoch, validation_data=([X_train_val[&#39;left&#39;], X_train_val[&#39;right&#39;]], Y_train_val),callbacks=[cp_callback]) print(&quot;Training time finished. n{} epochs in {}&quot;.format(n_epoch, datetime.timedelta(seconds=time()-training_start_time))) . sns.set() plt.plot(malstm_trained.history[&#39;loss&#39;],&#39;r-.&#39;,label=&#39;loss&#39;) plt.plot(malstm_trained.history[&#39;val_loss&#39;],&#39;g-*&#39;,label=&#39;val_loss&#39;) plt.legend() . plt.plot(malstm_trained.history[&#39;auc&#39;],&#39;r-.&#39;,label=&#39;auc&#39;) plt.plot(malstm_trained.history[&#39;val_auc&#39;],&#39;g-*&#39;,label=&#39;val_auc&#39;) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Auc&#39;) plt.legend() . from sklearn.metrics import confusion_matrix from matplotlib import pyplot as plt import itertools def plot_confusion_matrix(cm,classes,normalize=False,title=&#39;confusion matrix&#39;,cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;)/cm.sum(axis=1)[:,np.newaxis] print(&#39;normalized&#39;) else: print(&#39;not normalized&#39;) # print(cm) plt.imshow(cm,interpolation=&#39;nearest&#39;,cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(-1,len(classes)) plt.xticks(tick_marks,np.array(classes)-1,rotation=45) plt.yticks(tick_marks,np.array(classes)-1) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max()/2.0 for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])): plt.text(j,i,format(cm[i,j],fmt),horizontalalignment=&#39;center&#39;,color=&#39;white&#39; if cm[i,j]&gt;thresh else &#39;black&#39;) plt.tight_layout() plt.ylabel(&#39;True Label&#39;) plt.xlabel(&#39;Predicted Label&#39;) plt.show() . path = &#39;cp_epoch_0055.ckpt&#39; malstm.load_weights(path) . p_test = malstm.predict([X_train_val[&#39;left&#39;], X_train_val[&#39;right&#39;]]) p_test_pd = pd.DataFrame(p_test,columns=[&#39;prob&#39;]) . p_test_pd.loc[p_test_pd[&#39;prob&#39;]&gt;=0.5,&#39;prob&#39;] = int(1) . p_test_pd.loc[p_test_pd[&#39;prob&#39;]&lt;0.5,&#39;prob&#39;] = int(0) . cm = confusion_matrix(Y_train_val,p_test_pd.prob) . plot_confusion_matrix(cm,list(range(3))) . import sklearn sklearn.metrics.f1_score(Y_train_val,p_test_pd.prob) .",
            "url": "https://rasitabay.github.io/blog/collision%20risk/satellites/deep%20learning/manhattan-lstm/2020/04/22/ESA-Collision-Risk-Prediction.html",
            "relUrl": "/collision%20risk/satellites/deep%20learning/manhattan-lstm/2020/04/22/ESA-Collision-Risk-Prediction.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "High Precision 6D Orbit Propagator",
            "content": "1. Orbit Propagator . This blog post introduces a high precision 6D orbit propagator that I developed for my research. This 6D orbit propagator leverages Orekit low-level spaceflight dynamics library (3D dynamics only) and CNES VTS (Celestia) visualization tool. Below is the structure of the proposed 6D flight dynamics library (Figure 1). . . Figure 1: Layout of the proposed high precision 6D orbit propagator.There are 6 classes that handles the associated tasks. State class contains all functions and variables that defines the physical shape, size, and material properties of space objects, celestial bodies, frame definitions and conversions. Force class includes functions that compute the conservative, such as non-aspherical gravitational potential and third-body mass, and non-conservative forces, such as solar radiation pressure and air drag. Torque class contains functions for computing gravity gradient torque for gravitational forces and solar radiation pressure and air drag torque for non-gravitational forces. Propagator class includes functions to propagate the orbital equations of motion in Newtonian formalism using adaptive numerical integrators such as Runge-Kutta integrator of order 8 or Dormand and Prince of order 8. Visibility class contains the functions for various methods to compute the bidirectional reflectance distribution function (BRDF) of the space objects based on visibility geometry. All reference frames use the IERS 2010 conventions, and earth orientation parameters (EOP) are updated regularly. . 2. Demo Setup . For the demo, two extreme cases in terms of sensitivity of the space objects, namely massive cannonball and a small MLI-type material that is wrinkled, to the non-gravitational forces are presented in the demo. Cannonball is less sensitive to the non-gravitational forces, whereas non-gravitational forces and torques drastically affect the attitude/orbit evolution of high area-to-mass ratio wrinkled sheet, which is composed of 10 flat surfaces. Figure 2 shows the shapes of the space objects that are selected for the demo. Table 1 shows some of the physical parameters used for the demo. In Table 1, $d$ and $s$ are fractions of the diffuse and specular reflection ($s+d=1$), $ rho$ and $F_{0}$ are diffuse reflectance and specular reflectance of the surface at normal incidence, and $n_{u}$ and $n_{v}$ are phong-like exponents that control the shape of the specular lobe. . . Figure 2 : Shapes of cannonball and HAMR object that are used for the demo. Properties Cannonball HAMR object . Mass | 7850 kg | 0.0001125 kg | . Area | 1.209 $m^2$ | 0.0025 $m^2$ (each facet) | . Sunlight intensity | 455 $W/m^2$ | 455 $W/m^2$ | . d | 1.0 | 0.5 | . s | 0.0 | 0.5 | . $ rho$ | 0.26 | 0.26 | . $F_{0}$ | - | 0.5 | . $n_{u}$ | - | 0.5 | . $n_{v}$ | - | 0.5 | . Table 1 : Physical model parameters used for the space objects.For the initial orbit, the orbital elemets of a satellite (OPS 5111) are used by transforming them from TEME to J2000 reference frames. First, a two-line element set is fetched from spacetrack.org, and then the state (cartesian coordinates) at epoch is converted to J2000. The TLE used is , . 0 OPS 5111 . 1 10684U 78020A 20110.24021950 +.00000083 +00000-0 +00000-0 0 9994 . 2 10684 063.0042 120.7837 0079708 206.3069 153.3306 01.98073183291879 . The air drag coefficient is $2.8$ (for both objects) and aspherical gravitational potential order and degree are 20. The initial rotation rate for the HAMR object is $0.5$ degrees/s around the orbital normal axis. . 3. Results . Figure 3 and Figure 4 show the light intensity observed by the ground telescope for the hamr object and the cannonball respectively. Figure 5 shows the relative orbital evolution starting from the epoch (1 day propagation starting at 2020-04-19T05:45:54.964 (UTC)). Please note that the time window for the visibility starts (for the location at lat: $57.0^{ circ}$ and long: $-5.0^{ circ}$) at 2020-04-19T21:17:04.964 (UTC) and ends at 2020-04-19T22:55:24.964 (UTC). . . Figure 3 : Light intensity observed by the ground telescope for hamr object. . Figure 4 : Light intensity observed by the ground telescope for cannonball. . Figure 5 : Relative orbital evolution of space objects.The 3D visualisation for the relative motion (orbits diverge because hamr object is drastically perturbed by the non-gravitational forces) can be reached from the link, . https://drive.google.com/file/d/1FeZUaFtIT5W8D5UL9v2Mr_PMCZL0Nqz8/view?usp=sharing . And the visible pass in 3D (visible pass occurs when the ground observer is in dark and space object is in sun light) can be reached from the link, . https://drive.google.com/file/d/14F6Q6MH2FCJmfes40f9W3-kjYU7av1Ee/view?usp=sharing. . 4. References . [1] Space-track.org (fetched TLE for the initial orbit), https://www.space-track.org/auth/login . [2] Orekit (used for frame transformations and gravitational potential computations), https://www.orekit.org/ . [3] CNES VTS (used for 3D visualisation), https://timeloop.fr/vts/ . [4] Ashikhmin, Michael and Shirley, Peter, An Anisotropic Phong BRDF Model, Journal of Graphics Tools, 2000. . 5. Code . The code used to generate above analysis is provided below. . # Call the required libraries import seaborn as sns import numpy as np from matplotlib import pyplot from state import State from visibility import LambertianSphere # Replace ashikhminPremoze with LambertianSphere for cannonball from forces import NeutralDrag from forces import HolmesFeatherstoneGravity from forces import ThirdBodyForce from forces import ImprovedRadiation from torques import Torques from torques import GravityGradient from torques import MagneticDipole from propagator import NumericalPropagator from sgp4.io import twoline2rv from sgp4.earth_gravity import wgs84 import utils as utl . # Initial orbit is transformed from TLE mean elements in TEME to cartesian states in J2000. TLE=[] TLE.append(&#39;0 OPS 5111&#39;) TLE.append(&#39;1 10684U 78020A 20110.24021950 +.00000083 +00000-0 +00000-0 0 9994&#39;) TLE.append(&#39;2 10684 063.0042 120.7837 0079708 206.3069 153.3306 01.98073183291879&#39;)#Eccentricity modified satellite = twoline2rv( TLE[1],TLE[2],wgs84) tlepos = satellite.propagatemin(0)[0] tlevel = satellite.propagatemin(0)[1] tleTime = str(satellite.epoch.strftime(&quot;%Y-%m-%dT%H:%M:%S.%f&quot;)[0:-3]) state = np.array([tlepos[0],tlepos[1],tlepos[2],tlevel[0],tlevel[1],tlevel[2]])*1000.0 stateNew = utl.rotateFrame(tleTime,state,&#39;TEME&#39;,&#39;J2000&#39;) . # The configuration for Steel cannonball all diffusive and HAMR object with diffuse and specular reflection spacecraftConfig = {} spacecraftConfig[&#39;name&#39;] = &#39;Steel&#39; spacecraftConfig[&#39;type&#39;] = &#39;3DoF&#39;#6DoF or 3DoF spacecraftConfig[&#39;mass&#39;] = 7850#kg spacecraftConfig[&#39;date&#39;] = tleTime#UTC time spacecraftConfig[&#39;orbit&#39;] = np.array([stateNew[0] ,stateNew[1] ,stateNew[2], stateNew[3] ,stateNew[4] ,stateNew[5]])#PV in m and m/s (J2000) # Below is requred for 3DoF propagation - change type to 3DoF as well spacecraftConfig[&#39;radius&#39;] = 0.6204#m spacecraftState = State(spacecraftConfig) # # Below is the configuration for a HAMR object # spacecraftConfig[&#39;name&#39;] = &#39;Hamr&#39; # spacecraftConfig[&#39;type&#39;] = &#39;6DoF&#39;#6DoF or 3DoF # spacecraftConfig[&#39;mass&#39;] = 0.0001125#kg # spacecraftConfig[&#39;date&#39;] = tleTime#UTC time # spacecraftConfig[&#39;orbit&#39;] = np.array([stateNew[0] ,stateNew[1] ,stateNew[2], # stateNew[3] ,stateNew[4] ,stateNew[5]])#PV in m and m/s (J2000) # # Below is requred for 3DoF propagation - change type to 3DoF as well # spacecraftConfig[&#39;radius&#39;] = 0.3#m # # Below is requred for 6DoF propagation - change type to 6DoF as well # spacecraftConfig[&#39;attitude&#39;] = np.array([20.0,20.0,0.0,0.0,0.0,0.5])#omega and omega dot w.r.t orbit frame # spacecraftConfig[&#39;moi&#39;] = np.array([[1.54180631e-07, 0.0, -1.08223763e-07], # [0.0, 2.31268163e-07, 0.0], # [-1.08223763e-07, 0.0, 8.41187813e-08]])#m^2 w.r.t body frame # spacecraftConfig[&#39;magDipole&#39;] = np.array([0.01,0.01,0.01])#Nm/Tesla = Am^2 # spacecraftConfig[&#39;comOffset&#39;] = np.array([0.0,0.0,0.0])#m w.r.t body frame # spacecraftConfig[&#39;centerOfPressure&#39;] = np.array([[0.0,0.0,0.0558 ,0.0558 ,0.02165 ,0.02165 , # -0.02165,-0.02165,-0.0558 ,-0.0558], # [0.0,0.0,0.0 ,0.0 ,0.0 ,0.0 , # 0.0 ,0.0 ,0.0 ,0.0], # [0.0,0.0,0.07165,0.07165,0.0375 ,0.0375 , # -0.0375 ,-0.0375 ,-0.07165 ,-0.07165]]) # # location of facet center w.r.t body frame - meters # spacecraftConfig[&#39;areaOfFacets&#39;] = np.array([0.0025,0.0025,0.0025,0.0025,0.0025,0.0025,0.0025,0.0025, # 0.0025,0.0025]) # # area of each facet - meters^2 # spacecraftConfig[&#39;orientationOfFacets&#39;] = np.array([[0.0,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ], # [0.0,180.0,30.0 ,210.0,60.0,240.0,60.0,240.0,30.0,210.0], # [0.0,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ]]) # # orientation of the facet w.r.t body frame - degrees # spacecraftState = State(spacecraftConfig) . # Defines the airdrag parameters dragCoeff = 2.8 dragNeutral = NeutralDrag(spacecraftState,dragCoeff) . # Defines the aspherical gravitational potential parameters order = 20 degree = 20 gravity = HolmesFeatherstoneGravity(spacecraftState,order,degree) . # Defines the third-bosy mass interaction parameters sunBody = spacecraftState.sun moonBody = spacecraftState.moon thirdBodies = ThirdBodyForce(spacecraftState,[sunBody,moonBody]) . # Defines the visibility parameters gs = [57.0, -5.0, 606.0]#latitude, longitude, altitude brdfCoeff = {} brdfCoeff[&#39;cSunVis&#39;] = 455#W/m^2 brdfCoeff[&#39;d&#39;] = 0.5 brdfCoeff[&#39;rho&#39;] = 0.26 brdfCoeff[&#39;s&#39;] = 0.5 brdfCoeff[&#39;Fo&#39;] = 0.5 brdfCoeff[&#39;nu&#39;] = 0.5 brdfCoeff[&#39;nv&#39;] = 0.5 # AshikhminPremoze = ashikhminPremoze(spacecraftState,gs,brdfCoeff) # Below is when cannonball option LambertianSphere = LambertianSphere(spacecraftState,gs,brdfCoeff) . # SRPClassical = ClassicalRadiation(spacecraftState,brdfCoeff) . # Initializes the SRP force class emissivity = 0.05 SRPImproved = ImprovedRadiation(spacecraftState,brdfCoeff,emissivity) . # Initializes the torque classes # aeroTorque = Torques(spacecraftState,dragNeutral) # radiationTorque = Torques(spacecraftState,SRPImproved) # ggTorque = GravityGradient(spacecraftState,[])#No need for a force Model # magneticTorque = MagneticDipole(spacecraftState,[])#No need for a force Model . # Defines the propagator #prop = NumericalPropagator(8640.0,100.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPClassical], # [aeroTorque,radiationTorque,ggTorque,magneticTorque]) # prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPImproved], # [aeroTorque,radiationTorque],ashikhminPremoze) # prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, # [dragNeutral,gravity,thirdBodies,SRPImproved], # [],AshikhminPremoze) prop = NumericalPropagator(86400.0,10.0,spacecraftState,&#39;dop853&#39;, [dragNeutral,gravity,thirdBodies,SRPImproved], [],LambertianSphere) . # Propagate the orbit/attitude prop.Propagate() . # Keep steel data dataSteel = prop.sol[&#39;state&#39;][:,:6] # dataHamr = prop.sol[&#39;state&#39;][:,:6] . # Plots the flux seen by the ground observer selected = prop.sol[&#39;visibility&#39;][:,2]&gt;np.zeros(len(prop.sol[&#39;visibility&#39;][:])) desired = np.where(selected) startIndex = desired[0][0] endIndex = desired[0][-1] sns.set() print(&#39;Below plot is flux when start time : &#39; +str(prop.sol[&#39;time&#39;][startIndex]) + &#39; and end time : &#39;+str(prop.sol[&#39;time&#39;][endIndex])+&#39; (Cannonball object)&#39;) pyplot.plot(prop.sol[&#39;visibility&#39;][startIndex:endIndex,1]) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Flux (W/m^2)&#39;) pyplot.show() . Below plot is flux when start time : 2020-04-19T21:17:04.964 and end time : 2020-04-19T22:55:24.964 (Cannonball object) . # Plots the flux seen by the ground observer selected = prop.sol[&#39;visibility&#39;][:,2]&gt;np.zeros(len(prop.sol[&#39;visibility&#39;][:])) desired = np.where(selected) startIndex = desired[0][0] endIndex = desired[0][-1] sns.set() print(&#39;Below plot is flux when start time : &#39; +str(prop.sol[&#39;time&#39;][startIndex]) + &#39; and end time : &#39;+str(prop.sol[&#39;time&#39;][endIndex])+&#39; (HAMR object)&#39;) pyplot.plot(prop.sol[&#39;visibility&#39;][startIndex:endIndex,1]) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Flux (W/m^2)&#39;) pyplot.show() . Below plot is flux when start time : 2020-04-19T21:17:04.964 and end time : 2020-04-19T22:55:24.964 (HAMR object) . # Compute the relative orbit trajectory lenData = dataHamr.shape[0] relTrajectory = np.zeros((lenData,3)) posvel = np.zeros(6) for cnt1 in range(lenData): posvel[0:3] = dataSteel[cnt1,:3] posvel[3:6] = dataSteel[cnt1,3:] rotMatIne2Orb = utl.OrbitFrameAxes(posvel).T relTrajectory[cnt1,:] = np.dot(rotMatIne2Orb,dataHamr[cnt1,:3]-dataSteel[cnt1,:3])/1000.0 . # Plots the relative orbit trajectory pyplot.plot(relTrajectory[:,0],&#39;r&#39;,markersize=0.1, label=&quot;Along-Track&quot;) pyplot.plot(relTrajectory[:,1],&#39;g&#39;,markersize=0.1, label=&quot;Nadir&quot;) pyplot.plot(relTrajectory[:,2],&#39;b&#39;,markersize=0.1, label=&quot;Neg. Orbit Normal&quot;) pyplot.xlabel(&#39;Time (x10 seconds)&#39;) pyplot.ylabel(&#39;Reative Distance (km)&#39;) pyplot.legend(loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) pyplot.show() . # utl.createVTSSim(&#39;hamr&#39;, prop.sol[&#39;time&#39;], prop.sol[&#39;state&#39;][:,0:10], &#39;6DoF&#39;) utl.createVTSSim(&#39;steel&#39;, prop.sol[&#39;time&#39;], prop.sol[&#39;state&#39;][:,0:10], &#39;3DoF&#39;) .",
            "url": "https://rasitabay.github.io/blog/orbit%20propagator/hamr/6d/jupyter/2020/04/21/6D-Orbit-Propagation.html",
            "relUrl": "/orbit%20propagator/hamr/6d/jupyter/2020/04/21/6D-Orbit-Propagation.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rasitabay.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rasitabay.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rasitabay.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rasitabay.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}